"""
Malware Detection Security Plugin for REVENG

Plugin for detecting malware patterns and security threats.
"""

import os
import sys
import json
import hashlib
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

from ..base import SecurityPlugin, PluginMetadata, PluginContext, PluginCategory, PluginPriority
from ...core.errors import PluginError
from ...core.logger import get_logger

logger = get_logger()

class MalwareDetectionPlugin(SecurityPlugin):
    """Malware detection and security analysis plugin"""

    def get_metadata(self) -> PluginMetadata:
        """Get plugin metadata"""
        return PluginMetadata(
            name="malware_detection",
            version="1.0.0",
            description="Detects malware patterns, security threats, and suspicious behaviors",
            author="REVENG Team",
            category=PluginCategory.ML_SECURITY,
            priority=PluginPriority.HIGH,
            dependencies=[],
            requirements=["yara", "pefile"],
            tags=["security", "malware", "detection", "threat", "analysis"],
            homepage="https://github.com/reveng/reveng",
            license="MIT",
            min_reveng_version="1.0.0"
        )

    def initialize(self, context: PluginContext) -> bool:
        """Initialize the plugin"""
        try:
            # Check if required libraries are available
            try:
                import yara
                import pefile
                self.yara = yara
                self.pefile = pefile
            except ImportError as e:
                logger.error(f"Required libraries not available: {e}")
                return False

            # Initialize malware patterns
            self._initialize_malware_patterns()

            logger.info("Malware Detection plugin initialized")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize Malware Detection plugin: {e}")
            return False

    def _initialize_malware_patterns(self):
        """Initialize malware detection patterns"""

        try:
            # YARA rules for malware detection
            self.yara_rules = {
                "suspicious_apis": """
rule SuspiciousAPIs {
    strings:
        $api1 = "CreateRemoteThread" ascii
        $api2 = "WriteProcessMemory" ascii
        $api3 = "VirtualAllocEx" ascii
        $api4 = "URLDownloadToFile" ascii
        $api5 = "RegSetValueEx" ascii
    condition:
        3 of them
}""",
                "packed_executable": """
rule PackedExecutable {
    strings:
        $upx = "UPX!" ascii
        $fsg = "FSG!" ascii
        $mew = "MEW" ascii
    condition:
        any of them
}""",
                "crypto_indicators": """
rule CryptoIndicators {
    strings:
        $crypto1 = "CryptEncrypt" ascii
        $crypto2 = "CryptDecrypt" ascii
        $crypto3 = "CryptCreateHash" ascii
        $crypto4 = "CryptHashData" ascii
    condition:
        2 of them
}""",
                "network_indicators": """
rule NetworkIndicators {
    strings:
        $net1 = "socket" ascii
        $net2 = "connect" ascii
        $net3 = "send" ascii
        $net4 = "recv" ascii
        $net5 = "WSAStartup" ascii
    condition:
        3 of them
}"""
            }

            # Compile YARA rules
            self.compiled_rules = {}
            for rule_name, rule_content in self.yara_rules.items():
                try:
                    self.compiled_rules[rule_name] = self.yara.compile(source=rule_content)
                except Exception as e:
                    logger.warning(f"Failed to compile YARA rule {rule_name}: {e}")

            # Malware signatures
            self.malware_signatures = {
                "trojan": [
                    "backdoor", "trojan", "keylog", "steal", "spy", "monitor"
                ],
                "virus": [
                    "virus", "infect", "spread", "replicate", "payload"
                ],
                "worm": [
                    "worm", "network", "propagate", "spread", "autonomous"
                ],
                "rootkit": [
                    "rootkit", "hide", "stealth", "kernel", "driver"
                ],
                "ransomware": [
                    "ransom", "encrypt", "decrypt", "payment", "bitcoin"
                ]
            }

            # Suspicious API patterns
            self.suspicious_apis = {
                "process_injection": [
                    "CreateRemoteThread", "WriteProcessMemory", "VirtualAllocEx",
                    "NtCreateThreadEx", "RtlCreateUserThread"
                ],
                "persistence": [
                    "RegSetValueEx", "RegCreateKey", "CreateService", "StartService"
                ],
                "network_communication": [
                    "socket", "connect", "send", "recv", "WSAStartup", "InternetOpen"
                ],
                "file_operations": [
                    "CreateFile", "WriteFile", "DeleteFile", "MoveFile"
                ],
                "registry_manipulation": [
                    "RegOpenKey", "RegSetValue", "RegDeleteKey", "RegQueryValue"
                ],
                "anti_analysis": [
                    "IsDebuggerPresent", "CheckRemoteDebuggerPresent", "OutputDebugString"
                ]
            }

            logger.info("Malware patterns initialized")

        except Exception as e:
            logger.error(f"Failed to initialize malware patterns: {e}")
            raise

    def security_analysis(self, context: PluginContext) -> Dict[str, Any]:
        """Perform security analysis"""

        try:
            binary_path = context.binary_path
            if not Path(binary_path).exists():
                raise PluginError(f"Binary file not found: {binary_path}", plugin_name=self.metadata.name)

            logger.info(f"Performing security analysis on: {binary_path}")

            # Initialize results
            security_results = {
                "binary_path": binary_path,
                "analysis_timestamp": "2024-01-01T00:00:00Z",
                "threat_level": "UNKNOWN",
                "malware_family": "UNKNOWN",
                "confidence": 0.0,
                "indicators": [],
                "yara_matches": [],
                "api_analysis": {},
                "string_analysis": {},
                "entropy_analysis": {},
                "recommendations": []
            }

            # YARA rule matching
            yara_matches = self._run_yara_analysis(binary_path)
            security_results["yara_matches"] = yara_matches

            # API analysis
            api_analysis = self._analyze_apis(binary_path)
            security_results["api_analysis"] = api_analysis

            # String analysis
            string_analysis = self._analyze_strings(binary_path)
            security_results["string_analysis"] = string_analysis

            # Entropy analysis
            entropy_analysis = self._analyze_entropy(binary_path)
            security_results["entropy_analysis"] = entropy_analysis

            # Calculate threat level
            threat_level = self._calculate_threat_level(security_results)
            security_results["threat_level"] = threat_level

            # Identify malware family
            malware_family = self._identify_malware_family(security_results)
            security_results["malware_family"] = malware_family

            # Calculate confidence
            confidence = self._calculate_confidence(security_results)
            security_results["confidence"] = confidence

            # Generate recommendations
            recommendations = self._generate_recommendations(security_results)
            security_results["recommendations"] = recommendations

            # Save results
            output_dir = Path(context.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            results_file = output_dir / "security_analysis.json"
            with open(results_file, 'w') as f:
                json.dump(security_results, f, indent=2)

            logger.info(f"Security analysis completed: {threat_level} threat level, {malware_family} family")

            return {
                "security_analysis": security_results,
                "success": True,
                "output_file": str(results_file)
            }

        except Exception as e:
            logger.error(f"Security analysis failed: {e}")
            return {
                "security_analysis": {},
                "success": False,
                "error": str(e)
            }

    def _run_yara_analysis(self, binary_path: str) -> List[Dict[str, Any]]:
        """Run YARA rule analysis"""

        try:
            matches = []

            for rule_name, compiled_rule in self.compiled_rules.items():
                try:
                    yara_matches = compiled_rule.match(binary_path)
                    for match in yara_matches:
                        matches.append({
                            "rule_name": rule_name,
                            "rule_file": match.rule,
                            "tags": match.tags,
                            "meta": match.meta,
                            "strings": [str(s) for s in match.strings]
                        })
                except Exception as e:
                    logger.warning(f"YARA rule {rule_name} failed: {e}")

            return matches

        except Exception as e:
            logger.error(f"YARA analysis failed: {e}")
            return []

    def _analyze_apis(self, binary_path: str) -> Dict[str, Any]:
        """Analyze imported APIs for suspicious patterns"""

        try:
            api_analysis = {
                "suspicious_categories": {},
                "risk_score": 0.0,
                "total_apis": 0,
                "suspicious_apis": []
            }

            # Load PE file
            pe = self.pefile.PE(binary_path)

            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    dll_name = entry.dll.decode('utf-8', errors='ignore')

                    for imp in entry.imports:
                        if imp.name:
                            api_name = imp.name.decode('utf-8', errors='ignore')
                            api_analysis["total_apis"] += 1

                            # Check for suspicious APIs
                            for category, apis in self.suspicious_apis.items():
                                if api_name in apis:
                                    if category not in api_analysis["suspicious_categories"]:
                                        api_analysis["suspicious_categories"][category] = 0
                                    api_analysis["suspicious_categories"][category] += 1

                                    api_analysis["suspicious_apis"].append({
                                        "api": api_name,
                                        "dll": dll_name,
                                        "category": category,
                                        "risk_level": self._get_api_risk_level(category)
                                    })

            # Calculate risk score
            total_suspicious = sum(api_analysis["suspicious_categories"].values())
            if api_analysis["total_apis"] > 0:
                api_analysis["risk_score"] = (total_suspicious / api_analysis["total_apis"]) * 100

            return api_analysis

        except Exception as e:
            logger.error(f"API analysis failed: {e}")
            return {"error": str(e)}

    def _analyze_strings(self, binary_path: str) -> Dict[str, Any]:
        """Analyze strings for malware indicators"""

        try:
            string_analysis = {
                "malware_indicators": [],
                "suspicious_strings": [],
                "network_indicators": [],
                "file_indicators": [],
                "registry_indicators": []
            }

            # Extract strings (simplified implementation)
            with open(binary_path, 'rb') as f:
                data = f.read()

            # Simple string extraction
            strings = []
            current_string = ""
            for byte in data:
                if 32 <= byte <= 126:  # Printable ASCII
                    current_string += chr(byte)
                else:
                    if len(current_string) >= 4:
                        strings.append(current_string)
                    current_string = ""

            if len(current_string) >= 4:
                strings.append(current_string)

            # Analyze strings for malware indicators
            for string in strings:
                string_lower = string.lower()

                # Check for malware family indicators
                for family, indicators in self.malware_signatures.items():
                    for indicator in indicators:
                        if indicator in string_lower:
                            string_analysis["malware_indicators"].append({
                                "string": string,
                                "family": family,
                                "indicator": indicator
                            })

                # Check for suspicious patterns
                if any(pattern in string_lower for pattern in ["password", "secret", "key", "token"]):
                    string_analysis["suspicious_strings"].append({
                        "string": string,
                        "type": "sensitive_information"
                    })

                # Check for network indicators
                if any(pattern in string_lower for pattern in ["http://", "https://", "ftp://", "www."]):
                    string_analysis["network_indicators"].append({
                        "string": string,
                        "type": "network_communication"
                    })

                # Check for file indicators
                if any(pattern in string_lower for pattern in ["c:\\", "/home/", "/tmp/", ".exe", ".dll"]):
                    string_analysis["file_indicators"].append({
                        "string": string,
                        "type": "file_operation"
                    })

                # Check for registry indicators
                if any(pattern in string_lower for pattern in ["hkey_", "software\\", "system\\"]):
                    string_analysis["registry_indicators"].append({
                        "string": string,
                        "type": "registry_operation"
                    })

            return string_analysis

        except Exception as e:
            logger.error(f"String analysis failed: {e}")
            return {"error": str(e)}

    def _analyze_entropy(self, binary_path: str) -> Dict[str, Any]:
        """Analyze file entropy for packing indicators"""

        try:
            entropy_analysis = {
                "overall_entropy": 0.0,
                "high_entropy_regions": [],
                "packing_indicators": [],
                "compression_indicators": []
            }

            # Read file data
            with open(binary_path, 'rb') as f:
                data = f.read()

            # Calculate overall entropy
            entropy_analysis["overall_entropy"] = self._calculate_entropy(data)

            # Analyze entropy in chunks
            chunk_size = 1024
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                chunk_entropy = self._calculate_entropy(chunk)

                if chunk_entropy > 7.5:  # High entropy threshold
                    entropy_analysis["high_entropy_regions"].append({
                        "offset": i,
                        "size": len(chunk),
                        "entropy": chunk_entropy
                    })

            # Check for packing indicators
            if entropy_analysis["overall_entropy"] > 7.0:
                entropy_analysis["packing_indicators"].append("High overall entropy suggests packing")

            if len(entropy_analysis["high_entropy_regions"]) > 10:
                entropy_analysis["packing_indicators"].append("Multiple high entropy regions")

            return entropy_analysis

        except Exception as e:
            logger.error(f"Entropy analysis failed: {e}")
            return {"error": str(e)}

    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data"""

        if not data:
            return 0.0

        # Count byte frequencies
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1

        # Calculate entropy
        entropy = 0.0
        data_len = len(data)

        for count in byte_counts:
            if count > 0:
                probability = count / data_len
                entropy -= probability * (probability.bit_length() - 1)  # log2 approximation

        return entropy

    def _get_api_risk_level(self, category: str) -> str:
        """Get risk level for API category"""

        risk_levels = {
            "process_injection": "HIGH",
            "persistence": "MEDIUM",
            "network_communication": "MEDIUM",
            "file_operations": "LOW",
            "registry_manipulation": "LOW",
            "anti_analysis": "MEDIUM"
        }

        return risk_levels.get(category, "LOW")

    def _calculate_threat_level(self, security_results: Dict[str, Any]) -> str:
        """Calculate overall threat level"""

        try:
            threat_score = 0.0

            # YARA matches
            yara_matches = security_results.get("yara_matches", [])
            threat_score += len(yara_matches) * 2.0

            # API analysis
            api_analysis = security_results.get("api_analysis", {})
            threat_score += api_analysis.get("risk_score", 0.0) * 0.1

            # String analysis
            string_analysis = security_results.get("string_analysis", {})
            threat_score += len(string_analysis.get("malware_indicators", [])) * 1.0
            threat_score += len(string_analysis.get("suspicious_strings", [])) * 0.5

            # Entropy analysis
            entropy_analysis = security_results.get("entropy_analysis", {})
            if entropy_analysis.get("overall_entropy", 0) > 7.0:
                threat_score += 1.0

            # Determine threat level
            if threat_score >= 10.0:
                return "CRITICAL"
            elif threat_score >= 7.0:
                return "HIGH"
            elif threat_score >= 4.0:
                return "MEDIUM"
            elif threat_score >= 1.0:
                return "LOW"
            else:
                return "MINIMAL"

        except Exception as e:
            logger.error(f"Failed to calculate threat level: {e}")
            return "UNKNOWN"

    def _identify_malware_family(self, security_results: Dict[str, Any]) -> str:
        """Identify malware family"""

        try:
            family_scores = {}

            # Analyze string indicators
            string_analysis = security_results.get("string_analysis", {})
            malware_indicators = string_analysis.get("malware_indicators", [])

            for indicator in malware_indicators:
                family = indicator.get("family", "unknown")
                if family not in family_scores:
                    family_scores[family] = 0
                family_scores[family] += 1

            # Return family with highest score
            if family_scores:
                return max(family_scores, key=family_scores.get)
            else:
                return "UNKNOWN"

        except Exception as e:
            logger.error(f"Failed to identify malware family: {e}")
            return "UNKNOWN"

    def _calculate_confidence(self, security_results: Dict[str, Any]) -> float:
        """Calculate confidence score"""

        try:
            confidence = 0.0

            # YARA matches increase confidence
            yara_matches = security_results.get("yara_matches", [])
            confidence += len(yara_matches) * 0.2

            # API analysis confidence
            api_analysis = security_results.get("api_analysis", {})
            if api_analysis.get("risk_score", 0) > 50:
                confidence += 0.3

            # String analysis confidence
            string_analysis = security_results.get("string_analysis", {})
            if string_analysis.get("malware_indicators", []):
                confidence += 0.3

            # Entropy analysis confidence
            entropy_analysis = security_results.get("entropy_analysis", {})
            if entropy_analysis.get("overall_entropy", 0) > 7.0:
                confidence += 0.2

            return min(confidence, 1.0)

        except Exception as e:
            logger.error(f"Failed to calculate confidence: {e}")
            return 0.0

    def _generate_recommendations(self, security_results: Dict[str, Any]) -> List[str]:
        """Generate security recommendations"""

        try:
            recommendations = []

            threat_level = security_results.get("threat_level", "UNKNOWN")

            if threat_level in ["HIGH", "CRITICAL"]:
                recommendations.append("Immediate isolation and analysis required")
                recommendations.append("Do not execute on production systems")
                recommendations.append("Notify security team immediately")

            if threat_level in ["MEDIUM", "HIGH", "CRITICAL"]:
                recommendations.append("Perform additional static analysis")
                recommendations.append("Consider dynamic analysis in sandbox")
                recommendations.append("Review network communication patterns")

            # API-specific recommendations
            api_analysis = security_results.get("api_analysis", {})
            suspicious_categories = api_analysis.get("suspicious_categories", {})

            if "process_injection" in suspicious_categories:
                recommendations.append("Monitor for process injection attempts")

            if "persistence" in suspicious_categories:
                recommendations.append("Check registry and service persistence")

            if "network_communication" in suspicious_categories:
                recommendations.append("Monitor network traffic")

            # Entropy recommendations
            entropy_analysis = security_results.get("entropy_analysis", {})
            if entropy_analysis.get("overall_entropy", 0) > 7.0:
                recommendations.append("File appears to be packed - consider unpacking")

            return recommendations

        except Exception as e:
            logger.error(f"Failed to generate recommendations: {e}")
            return []

    def cleanup(self, context: PluginContext) -> bool:
        """Cleanup plugin resources"""
        try:
            logger.info("Malware Detection plugin cleanup completed")
            return True
        except Exception as e:
            logger.error(f"Failed to cleanup Malware Detection plugin: {e}")
            return False
