#!/usr/bin/env python3
"""
ML-Based Malware Family Classifier
==================================

Deep learning and machine learning models for malware family identification
and behavioral analysis for unknown threat detection.

Author: REVENG Project - AI Enhancement Module
Version: 1.0
"""

import json
import logging
import pickle
import hashlib
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from pathlib import Path
import re
from collections import Counter

try:
    from sklearn.ensemble import RandomForestClassifier, IsolationForest
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.decomposition import PCA
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learn not available - ML malware classification disabled")

try:
    # Optional: PyTorch for deep learning (if available)
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader, TensorDataset
    from torch.nn.utils.rnn import pad_sequence
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    logging.info("PyTorch not available - deep learning features disabled")

try:
    # Optional: Transformers for embedding models
    from transformers import AutoTokenizer, AutoModel
    import sentence_transformers
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.info("Transformers not available - embedding models disabled")

try:
    from .ai_enhanced_data_models import (
        MalwareClassification, BehavioralPattern, ThreatFamily,
        Evidence, ConfidenceLevel, EvidenceTracker
    )
except ImportError:
    from ai_enhanced_data_models import (
        MalwareClassification, BehavioralPattern, ThreatFamily,
        Evidence, ConfidenceLevel, EvidenceTracker
    )


@dataclass
class MalwareFeatures:
    """Feature vector for malware classification"""
    # File characteristics
    file_size: int = 0
    entropy: float = 0.0
    section_count: int = 0
    import_count: int = 0
    export_count: int = 0

    # String features
    string_count: int = 0
    url_count: int = 0
    ip_count: int = 0
    domain_count: int = 0

    # Behavioral features
    network_indicators: int = 0
    persistence_indicators: int = 0
    evasion_indicators: int = 0
    payload_indicators: int = 0

    # Code patterns
    packer_indicators: int = 0
    obfuscation_indicators: int = 0
    crypto_indicators: int = 0

    # API usage patterns
    file_api_calls: int = 0
    registry_api_calls: int = 0
    network_api_calls: int = 0
    process_api_calls: int = 0

    # Structural features
    function_count: int = 0
    basic_block_count: int = 0
    call_graph_complexity: float = 0.0

    # Metadata
    compilation_timestamp: int = 0
    has_debug_info: bool = False
    is_packed: bool = False
    is_signed: bool = False


class MalwareFeatureExtractor:
    """Extract features from malware samples for classification"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # Define behavioral patterns
        self.behavioral_patterns = {
            'network': [
                r'socket|connect|send|recv|http|ftp|tcp|udp',
                r'InternetOpen|InternetConnect|HttpSendRequest',
                r'WSAStartup|WSASocket|WSAConnect',
            ],
            'persistence': [
                r'CreateService|StartService|RegSetValue',
                r'SetWindowsHook|CreateFile.*startup',
                r'HKEY_.*Run|HKEY_.*RunOnce',
            ],
            'evasion': [
                r'VirtualProtect|WriteProcessMemory|CreateRemoteThread',
                r'SetThreadContext|NtUnmapViewOfSection',
                r'IsDebuggerPresent|CheckRemoteDebuggerPresent',
            ],
            'payload': [
                r'CreateProcess|ShellExecute|WinExec',
                r'LoadLibrary|GetProcAddress|VirtualAlloc',
                r'CopyFile|MoveFile|DeleteFile',
            ]
        }

        # Packer signatures
        self.packer_signatures = {
            'upx': [b'UPX!', b'UPX0', b'UPX1'],
            'aspack': [b'aPLib', b'ASPack'],
            'pecompact': [b'PECompact', b'pec1', b'pec2'],
            'vmprotect': [b'VMProtect', b'.vmp0', b'.vmp1'],
            'themida': [b'Themida', b'.themida'],
        }

        # Crypto indicators
        self.crypto_patterns = [
            r'CryptAcquireContext|CryptCreateHash|CryptEncrypt',
            r'aes|des|rsa|md5|sha1|sha256',
            r'encrypt|decrypt|cipher|hash',
        ]

        # Obfuscation indicators
        self.obfuscation_patterns = [
            r'[A-Za-z0-9]{20,}',  # Long random strings
            r'\\x[0-9a-fA-F]{2}',  # Hex encoded strings
            r'base64|b64decode',
            r'xor|rol|ror',  # Simple encryption
        ]

    def extract_features(self, file_path: str, strings_data: List[str] = None,
                        api_calls: List[str] = None, code_analysis: Dict = None) -> MalwareFeatures:
        """Extract comprehensive features from malware sample"""
        features = MalwareFeatures()

        try:
            # File-based features
            features = self._extract_file_features(file_path, features)

            # String-based features
            if strings_data:
                features = self._extract_string_features(strings_data, features)

            # API call features
            if api_calls:
                features = self._extract_api_features(api_calls, features)

            # Code analysis features
            if code_analysis:
                features = self._extract_code_features(code_analysis, features)

            return features

        except Exception as e:
            self.logger.error(f"Error extracting malware features: {e}")
            return features

    def _extract_file_features(self, file_path: str, features: MalwareFeatures) -> MalwareFeatures:
        """Extract file-based features"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                return features

            # Basic file properties
            features.file_size = file_path.stat().st_size

            # Calculate entropy
            with open(file_path, 'rb') as f:
                data = f.read()
                features.entropy = self._calculate_entropy(data)

            # Check for packer signatures
            features.is_packed = self._detect_packer(data)
            features.packer_indicators = sum(
                1 for packer_sigs in self.packer_signatures.values()
                for sig in packer_sigs
                if sig in data
            )

            return features

        except Exception as e:
            self.logger.error(f"Error extracting file features: {e}")
            return features

    def _extract_string_features(self, strings_data: List[str], features: MalwareFeatures) -> MalwareFeatures:
        """Extract features from strings"""
        features.string_count = len(strings_data)

        # Count different types of strings
        url_pattern = re.compile(r'https?://[^\s<>"{}|\\^`\[\]]+')
        ip_pattern = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')
        domain_pattern = re.compile(r'\b[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*\.[a-zA-Z]{2,}\b')

        all_strings = ' '.join(strings_data)

        features.url_count = len(url_pattern.findall(all_strings))
        features.ip_count = len(ip_pattern.findall(all_strings))
        features.domain_count = len(domain_pattern.findall(all_strings))

        # Count behavioral indicators
        for behavior_type, patterns in self.behavioral_patterns.items():
            count = 0
            for pattern in patterns:
                count += len(re.findall(pattern, all_strings, re.IGNORECASE))

            if behavior_type == 'network':
                features.network_indicators = count
            elif behavior_type == 'persistence':
                features.persistence_indicators = count
            elif behavior_type == 'evasion':
                features.evasion_indicators = count
            elif behavior_type == 'payload':
                features.payload_indicators = count

        # Count crypto and obfuscation indicators
        features.crypto_indicators = sum(
            len(re.findall(pattern, all_strings, re.IGNORECASE))
            for pattern in self.crypto_patterns
        )

        features.obfuscation_indicators = sum(
            len(re.findall(pattern, all_strings, re.IGNORECASE))
            for pattern in self.obfuscation_patterns
        )

        return features

    def _extract_api_features(self, api_calls: List[str], features: MalwareFeatures) -> MalwareFeatures:
        """Extract features from API calls"""
        api_categories = {
            'file': ['CreateFile', 'ReadFile', 'WriteFile', 'DeleteFile', 'CopyFile', 'MoveFile'],
            'registry': ['RegOpenKey', 'RegSetValue', 'RegQueryValue', 'RegDeleteKey'],
            'network': ['socket', 'connect', 'send', 'recv', 'InternetOpen', 'HttpSendRequest'],
            'process': ['CreateProcess', 'OpenProcess', 'TerminateProcess', 'CreateThread'],
        }

        api_counter = Counter(api_calls)

        for category, api_list in api_categories.items():
            count = sum(api_counter.get(api, 0) for api in api_list)

            if category == 'file':
                features.file_api_calls = count
            elif category == 'registry':
                features.registry_api_calls = count
            elif category == 'network':
                features.network_api_calls = count
            elif category == 'process':
                features.process_api_calls = count

        return features

    def _extract_code_features(self, code_analysis: Dict, features: MalwareFeatures) -> MalwareFeatures:
        """Extract features from code analysis results"""
        features.function_count = code_analysis.get('function_count', 0)
        features.basic_block_count = code_analysis.get('basic_block_count', 0)
        features.call_graph_complexity = code_analysis.get('call_graph_complexity', 0.0)
        features.import_count = code_analysis.get('import_count', 0)
        features.export_count = code_analysis.get('export_count', 0)
        features.section_count = code_analysis.get('section_count', 0)

        return features

    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data"""
        if not data:
            return 0.0

        # Count byte frequencies
        byte_counts = Counter(data)
        data_len = len(data)

        # Calculate entropy
        entropy = 0.0
        for count in byte_counts.values():
            probability = count / data_len
            if probability > 0:
                entropy -= probability * np.log2(probability)

        return entropy

    def _detect_packer(self, data: bytes) -> bool:
        """Detect if file is packed"""
        # Check for packer signatures
        for packer_sigs in self.packer_signatures.values():
            for sig in packer_sigs:
                if sig in data:
                    return True

        # Check entropy (packed files typically have high entropy)
        entropy = self._calculate_entropy(data)
        if entropy > 7.0:  # High entropy threshold
            return True

        return False


class DeepMalwareClassifier(nn.Module):
    """Deep neural network for malware family classification"""

    def __init__(self, input_size: int, num_families: int, hidden_sizes: List[int] = None):
        super(DeepMalwareClassifier, self).__init__()

        if hidden_sizes is None:
            hidden_sizes = [512, 256, 128, 64]

        layers = []
        prev_size = input_size

        # Build hidden layers with dropout and batch normalization
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_size = hidden_size

        # Output layer
        layers.append(nn.Linear(prev_size, num_families))

        self.network = nn.Sequential(*layers)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        logits = self.network(x)
        return logits

    def predict_proba(self, x):
        self.eval()  # Set to evaluation mode
        with torch.no_grad():
            logits = self.forward(x)
            return self.softmax(logits)


class BehavioralAnalysisRNN(nn.Module):
    """RNN for behavioral sequence analysis"""

    def __init__(self, vocab_size: int, embedding_dim: int = 128,
                 hidden_dim: int = 256, num_layers: int = 2, num_classes: int = 10):
        super(BehavioralAnalysisRNN, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                           batch_first=True, dropout=0.3, bidirectional=True)
        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, x, lengths=None):
        # x shape: (batch_size, seq_len)
        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)

        # LSTM processing
        lstm_out, (hidden, cell) = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim*2)

        # Attention mechanism
        lstm_out = lstm_out.transpose(0, 1)  # (seq_len, batch_size, hidden_dim*2)
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        attn_out = attn_out.transpose(0, 1)  # (batch_size, seq_len, hidden_dim*2)

        # Global max pooling
        pooled = torch.max(attn_out, dim=1)[0]  # (batch_size, hidden_dim*2)

        # Classification
        output = self.classifier(pooled)
        return output


class MalwareEmbeddingModel(nn.Module):
    """Embedding model for malware similarity analysis"""

    def __init__(self, input_size: int, embedding_dim: int = 256):
        super(MalwareEmbeddingModel, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(256, embedding_dim),
            nn.BatchNorm1d(embedding_dim),
            nn.Tanh()  # Normalize embeddings to [-1, 1]
        )

    def forward(self, x):
        return self.encoder(x)

    def get_embedding(self, x):
        """Get normalized embedding vector"""
        self.eval()  # Set to evaluation mode to avoid batch norm issues
        with torch.no_grad():
            embedding = self.forward(x)
            # L2 normalize
            return F.normalize(embedding, p=2, dim=1)


class ContrastiveLoss(nn.Module):
    """Contrastive loss for similarity learning"""

    def __init__(self, margin: float = 1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, embedding1, embedding2, label):
        """
        Args:
            embedding1, embedding2: Normalized embeddings
            label: 1 if similar, 0 if dissimilar
        """
        euclidean_distance = F.pairwise_distance(embedding1, embedding2)

        loss_contrastive = torch.mean(
            label * torch.pow(euclidean_distance, 2) +
            (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        )

        return loss_contrastive


class AdvancedMalwareDataset(Dataset):
    """Dataset for advanced malware classification"""

    def __init__(self, features: List[List[float]], labels: List[str],
                 sequences: List[List[int]] = None, family_to_idx: Dict[str, int] = None):
        self.features = torch.FloatTensor(features)

        # Create family to index mapping if not provided
        if family_to_idx is None:
            unique_families = list(set(labels))
            self.family_to_idx = {family: idx for idx, family in enumerate(unique_families)}
        else:
            self.family_to_idx = family_to_idx

        self.labels = torch.LongTensor([self.family_to_idx[label] for label in labels])

        # Optional behavioral sequences
        if sequences:
            max_len = max(len(seq) for seq in sequences)
            self.sequences = torch.zeros(len(sequences), max_len, dtype=torch.long)
            for i, seq in enumerate(sequences):
                self.sequences[i, :len(seq)] = torch.LongTensor(seq)
        else:
            self.sequences = None

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        item = {
            'features': self.features[idx],
            'label': self.labels[idx]
        }

        if self.sequences is not None:
            item['sequence'] = self.sequences[idx]

        return item


class MalwareFamilyDatabase:
    """Database of known malware families and their characteristics"""

    def __init__(self):
        self.families = {
            'apt1': {
                'aliases': ['Comment Crew', 'PLA Unit 61398'],
                'characteristics': {
                    'network_indicators': ['comment.php', 'login.php', 'index.asp'],
                    'file_patterns': ['temp.exe', 'svchost.exe'],
                    'crypto_usage': 'moderate',
                    'persistence_methods': ['registry_run_keys', 'services'],
                },
                'behavioral_signature': [0.8, 0.6, 0.4, 0.7, 0.5, 0.3, 0.6, 0.4]
            },
            'apt28': {
                'aliases': ['Fancy Bear', 'Sofacy', 'Sednit'],
                'characteristics': {
                    'network_indicators': ['bit.ly', 'tinyurl.com'],
                    'file_patterns': ['flash*.exe', 'adobe*.exe'],
                    'crypto_usage': 'high',
                    'persistence_methods': ['registry_run_keys', 'scheduled_tasks'],
                },
                'behavioral_signature': [0.9, 0.8, 0.7, 0.6, 0.8, 0.5, 0.7, 0.6]
            },
            'lazarus': {
                'aliases': ['Hidden Cobra', 'Guardians of Peace'],
                'characteristics': {
                    'network_indicators': ['.tk', '.ml'],
                    'file_patterns': ['update*.exe', 'install*.exe'],
                    'crypto_usage': 'very_high',
                    'persistence_methods': ['mbr_modification', 'bootkit'],
                },
                'behavioral_signature': [0.7, 0.9, 0.8, 0.9, 0.6, 0.8, 0.9, 0.7]
            },
            'fin7': {
                'aliases': ['Carbanak', 'Navigator Group'],
                'characteristics': {
                    'network_indicators': ['invoice', 'payment'],
                    'file_patterns': ['invoice*.exe', 'payment*.exe'],
                    'crypto_usage': 'moderate',
                    'persistence_methods': ['registry_run_keys', 'com_hijacking'],
                },
                'behavioral_signature': [0.6, 0.7, 0.5, 0.8, 0.7, 0.4, 0.6, 0.8]
            },
            'emotet': {
                'aliases': ['Geodo', 'Mealybug'],
                'characteristics': {
                    'network_indicators': ['doc', 'docx', 'macro'],
                    'file_patterns': ['*.doc', '*.docx'],
                    'crypto_usage': 'high',
                    'persistence_methods': ['registry_run_keys', 'scheduled_tasks'],
                },
                'behavioral_signature': [0.5, 0.8, 0.6, 0.7, 0.9, 0.6, 0.5, 0.7]
            },
            'wannacry': {
                'aliases': ['WannaCrypt', 'WCry'],
                'characteristics': {
                    'network_indicators': ['killswitch', 'onion'],
                    'file_patterns': ['@WanaDecryptor@*', 'tasksche.exe'],
                    'crypto_usage': 'very_high',
                    'persistence_methods': ['smb_exploit', 'file_encryption'],
                },
                'behavioral_signature': [0.3, 0.4, 0.2, 0.9, 0.3, 0.9, 0.8, 0.9]
            }
        }

    def get_family_signature(self, family_name: str) -> Optional[List[float]]:
        """Get behavioral signature for malware family"""
        family = self.families.get(family_name.lower())
        return family['behavioral_signature'] if family else None

    def get_all_signatures(self) -> Dict[str, List[float]]:
        """Get all family signatures"""
        return {name: data['behavioral_signature']
                for name, data in self.families.items()}


class MLMalwareClassifier:
    """Advanced machine learning-based malware family classifier with deep learning"""

    def __init__(self, model_dir: str = "models"):
        self.logger = logging.getLogger(__name__)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)

        self.feature_extractor = MalwareFeatureExtractor()
        self.family_database = MalwareFamilyDatabase()
        self.evidence_tracker = EvidenceTracker()

        # Initialize models
        self.models = {}
        self.vectorizers = {}
        self.scalers = {}
        self.deep_models = {}

        # Deep learning parameters
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.feature_dim = 26  # Number of features in feature vector
        self.embedding_dim = 256
        self.num_families = 20  # Will be updated based on training data

        # Behavioral sequence parameters
        self.vocab_size = 10000  # API call vocabulary size
        self.max_seq_length = 1000

        # Family mappings
        self.family_to_idx = {}
        self.idx_to_family = {}

        if not SKLEARN_AVAILABLE:
            self.logger.error("scikit-learn not available - ML classification disabled")
            return

        # Initialize model pipelines
        self._initialize_models()

        # Initialize deep learning models
        if PYTORCH_AVAILABLE:
            self._initialize_deep_models()

        # Load pre-trained models if available
        self._load_models()

    def _initialize_models(self):
        """Initialize ML model pipelines"""
        if not SKLEARN_AVAILABLE:
            return

        # Family classification model
        self.models['family_classifier'] = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                random_state=42
            ))
        ])

        # Anomaly detection for unknown threats
        self.models['anomaly_detector'] = Pipeline([
            ('scaler', StandardScaler()),
            ('detector', IsolationForest(
                contamination=0.1,
                random_state=42
            ))
        ])

        # Behavioral clustering
        self.models['behavior_clusterer'] = Pipeline([
            ('scaler', StandardScaler()),
            ('pca', PCA(n_components=8)),
            ('clusterer', KMeans(n_clusters=6, random_state=42))
        ])

        # String-based classifier
        self.vectorizers['string_vectorizer'] = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),
            analyzer='char'
        )

        self.models['string_classifier'] = RandomForestClassifier(
            n_estimators=100,
            random_state=42
        )

    def _initialize_deep_models(self):
        """Initialize deep learning models"""
        if not PYTORCH_AVAILABLE:
            return

        try:
            # Deep malware family classifier
            self.deep_models['deep_classifier'] = DeepMalwareClassifier(
                input_size=self.feature_dim,
                num_families=self.num_families
            ).to(self.device)

            # Behavioral analysis RNN
            self.deep_models['behavioral_rnn'] = BehavioralAnalysisRNN(
                vocab_size=self.vocab_size,
                embedding_dim=128,
                hidden_dim=256,
                num_layers=2,
                num_classes=10  # Behavioral categories
            ).to(self.device)

            # Embedding model for similarity analysis
            self.deep_models['embedding_model'] = MalwareEmbeddingModel(
                input_size=self.feature_dim,
                embedding_dim=self.embedding_dim
            ).to(self.device)

            # Initialize optimizers
            self.optimizers = {
                'deep_classifier': optim.Adam(
                    self.deep_models['deep_classifier'].parameters(),
                    lr=0.001, weight_decay=1e-5
                ),
                'behavioral_rnn': optim.Adam(
                    self.deep_models['behavioral_rnn'].parameters(),
                    lr=0.001, weight_decay=1e-5
                ),
                'embedding_model': optim.Adam(
                    self.deep_models['embedding_model'].parameters(),
                    lr=0.001, weight_decay=1e-5
                )
            }

            # Loss functions
            self.loss_functions = {
                'classification': nn.CrossEntropyLoss(),
                'contrastive': ContrastiveLoss(margin=1.0)
            }

            self.logger.info("Deep learning models initialized successfully")

        except Exception as e:
            self.logger.error(f"Error initializing deep models: {e}")
            self.deep_models = {}

    def _load_models(self):
        """Load pre-trained models from disk"""
        for model_name in self.models.keys():
            model_file = self.model_dir / f"{model_name}_model.pkl"
            if model_file.exists():
                try:
                    # Use joblib for safer model loading
                    import joblib
                    self.models[model_name] = joblib.load(model_file)
                    self.logger.info(f"Loaded pre-trained model: {model_name}")
                except Exception as e:
                    self.logger.warning(f"Failed to load model {model_name}: {e}")

        # Load vectorizers
        for vec_name in self.vectorizers.keys():
            vec_file = self.model_dir / f"{vec_name}.pkl"
            if vec_file.exists():
                try:
                    # Use joblib for safer vectorizer loading
                    import joblib
                    self.vectorizers[vec_name] = joblib.load(vec_file)
                    self.logger.info(f"Loaded vectorizer: {vec_name}")
                except Exception as e:
                    self.logger.warning(f"Failed to load vectorizer {vec_name}: {e}")

    def classify_malware(self, file_path: str, strings_data: List[str] = None,
                        api_calls: List[str] = None, code_analysis: Dict = None) -> MalwareClassification:
        """Advanced malware classification using ensemble of ML and deep learning models"""
        if not SKLEARN_AVAILABLE:
            return MalwareClassification(
                family="unknown",
                confidence=0.0,
                is_malware=False,
                behavioral_patterns=[],
                similarity_scores={},
                classification_method="unavailable"
            )

        try:
            # Extract features
            features = self.feature_extractor.extract_features(
                file_path, strings_data, api_calls, code_analysis
            )

            # Convert to feature vector
            feature_vector = self._features_to_vector(features)

            # Traditional ML classification
            family_prediction = self._classify_family(feature_vector, strings_data)
            anomaly_score = self._detect_anomaly(feature_vector)
            behavioral_cluster = self._analyze_behavior(feature_vector)

            # Deep learning classification (if available)
            deep_prediction = None
            behavioral_analysis = None
            embedding_similarity = {}

            if PYTORCH_AVAILABLE and self.deep_models:
                deep_prediction = self._deep_classify_family(feature_vector)
                behavioral_analysis = self._analyze_behavioral_sequence(api_calls)
                embedding_similarity = self._calculate_embedding_similarity(feature_vector)

            # Ensemble predictions
            final_prediction = self._ensemble_predictions(
                family_prediction, deep_prediction, behavioral_analysis
            )

            # Calculate comprehensive similarity scores
            similarity_scores = self._calculate_similarity(feature_vector)
            if embedding_similarity:
                similarity_scores.update(embedding_similarity)

            # Determine if sample is malware
            is_malware = self._determine_malware_status(
                final_prediction, anomaly_score, features
            )

            # Create evidence
            evidence = self.evidence_tracker.add_evidence(
                "advanced_ml_malware_classification",
                f"Advanced ML classification completed for {Path(file_path).name}",
                "deep_learning",
                final_prediction['confidence'],
                {
                    "predicted_family": final_prediction['family'],
                    "anomaly_score": anomaly_score,
                    "behavioral_cluster": behavioral_cluster,
                    "deep_learning_used": PYTORCH_AVAILABLE and bool(self.deep_models),
                    "feature_count": len(feature_vector)
                }
            )

            # Extract behavioral patterns
            behavioral_patterns = self._extract_behavioral_patterns(features)
            if behavioral_analysis:
                behavioral_patterns.extend(behavioral_analysis.get('patterns', []))

            classification = MalwareClassification(
                family=final_prediction['family'],
                confidence=final_prediction['confidence'],
                is_malware=is_malware,
                behavioral_patterns=behavioral_patterns,
                similarity_scores=similarity_scores,
                classification_method="advanced_ensemble",
                anomaly_score=anomaly_score,
                behavioral_cluster=behavioral_cluster,
                evidence=[evidence]
            )

            return classification

        except Exception as e:
            self.logger.error(f"Error in advanced malware classification: {e}")
            return MalwareClassification(
                family="unknown",
                confidence=0.0,
                is_malware=False,
                behavioral_patterns=[],
                similarity_scores={},
                classification_method="error"
            )

    def _features_to_vector(self, features: MalwareFeatures) -> List[float]:
        """Convert features to numerical vector"""
        vector = [
            float(features.file_size) / 1000000,  # Normalize to MB
            features.entropy,
            float(features.section_count),
            float(features.import_count),
            float(features.export_count),
            float(features.string_count) / 1000,  # Normalize
            float(features.url_count),
            float(features.ip_count),
            float(features.domain_count),
            float(features.network_indicators),
            float(features.persistence_indicators),
            float(features.evasion_indicators),
            float(features.payload_indicators),
            float(features.packer_indicators),
            float(features.obfuscation_indicators),
            float(features.crypto_indicators),
            float(features.file_api_calls),
            float(features.registry_api_calls),
            float(features.network_api_calls),
            float(features.process_api_calls),
            float(features.function_count) / 100,  # Normalize
            float(features.basic_block_count) / 1000,  # Normalize
            features.call_graph_complexity,
            float(features.has_debug_info),
            float(features.is_packed),
            float(features.is_signed),
        ]
        return vector

    def _classify_family(self, feature_vector: List[float], strings_data: List[str] = None) -> Dict[str, Any]:
        """Classify malware family using ensemble of models"""
        try:
            # Feature-based classification
            family_model = self.models.get('family_classifier')
            if family_model and hasattr(family_model, 'predict_proba'):
                proba = family_model.predict_proba([feature_vector])[0]
                predicted_class = family_model.predict([feature_vector])[0]
                confidence = float(max(proba))
            else:
                predicted_class = "unknown"
                confidence = 0.0

            # String-based classification (if available)
            if strings_data and 'string_classifier' in self.models:
                string_text = ' '.join(strings_data)
                vectorizer = self.vectorizers.get('string_vectorizer')
                if vectorizer:
                    string_features = vectorizer.transform([string_text])
                    string_pred = self.models['string_classifier'].predict(string_features)[0]

                    # Combine predictions (simple ensemble)
                    if string_pred == predicted_class:
                        confidence = min(confidence + 0.1, 1.0)

            return {
                'family': predicted_class,
                'confidence': confidence
            }

        except Exception as e:
            self.logger.error(f"Error in family classification: {e}")
            return {'family': 'unknown', 'confidence': 0.0}

    def _detect_anomaly(self, feature_vector: List[float]) -> float:
        """Detect if sample is anomalous (unknown threat)"""
        try:
            anomaly_model = self.models.get('anomaly_detector')
            if anomaly_model:
                # Isolation Forest returns -1 for anomalies, 1 for normal
                anomaly_pred = anomaly_model.predict([feature_vector])[0]
                # Get anomaly score (lower = more anomalous)
                anomaly_score = anomaly_model.decision_function([feature_vector])[0]

                # Normalize to 0-1 range (higher = more anomalous)
                normalized_score = max(0, min(1, (0.5 - anomaly_score) / 1.0))
                return normalized_score

            return 0.5  # Default neutral score

        except Exception as e:
            self.logger.error(f"Error in anomaly detection: {e}")
            return 0.5

    def _analyze_behavior(self, feature_vector: List[float]) -> int:
        """Analyze behavioral patterns using clustering"""
        try:
            behavior_model = self.models.get('behavior_clusterer')
            if behavior_model:
                cluster = behavior_model.predict([feature_vector])[0]
                return int(cluster)

            return -1  # Unknown cluster

        except Exception as e:
            self.logger.error(f"Error in behavioral analysis: {e}")
            return -1

    def _calculate_similarity(self, feature_vector: List[float]) -> Dict[str, float]:
        """Calculate similarity to known malware families"""
        similarities = {}

        try:
            # Get known family signatures
            family_signatures = self.family_database.get_all_signatures()

            # Normalize feature vector to match signature length
            if len(feature_vector) >= 8:
                normalized_vector = feature_vector[:8]
            else:
                normalized_vector = feature_vector + [0.0] * (8 - len(feature_vector))

            # Calculate cosine similarity
            for family, signature in family_signatures.items():
                similarity = cosine_similarity([normalized_vector], [signature])[0][0]
                similarities[family] = float(similarity)

            return similarities

        except Exception as e:
            self.logger.error(f"Error calculating similarity: {e}")
            return {}

    def _determine_malware_status(self, family_prediction: Dict, anomaly_score: float,
                                 features: MalwareFeatures) -> bool:
        """Determine if sample is malware based on multiple indicators"""
        # High confidence family prediction
        if family_prediction['confidence'] > 0.8 and family_prediction['family'] != 'benign':
            return True

        # High anomaly score
        if anomaly_score > 0.7:
            return True

        # Behavioral indicators
        malware_indicators = (
            features.network_indicators +
            features.persistence_indicators +
            features.evasion_indicators +
            features.payload_indicators
        )

        if malware_indicators > 5:
            return True

        # High entropy (packed/encrypted)
        if features.entropy > 7.5 and features.is_packed:
            return True

        return False

    def _deep_classify_family(self, feature_vector: List[float]) -> Dict[str, Any]:
        """Classify malware family using deep neural network"""
        if not PYTORCH_AVAILABLE or 'deep_classifier' not in self.deep_models:
            return {'family': 'unknown', 'confidence': 0.0}

        try:
            model = self.deep_models['deep_classifier']
            model.eval()

            # Convert to tensor
            features_tensor = torch.FloatTensor([feature_vector]).to(self.device)

            # Get predictions
            with torch.no_grad():
                logits = model(features_tensor)
                probabilities = model.softmax(logits)

                # Get top prediction
                max_prob, predicted_idx = torch.max(probabilities, 1)
                predicted_family = self.idx_to_family.get(predicted_idx.item(), 'unknown')
                confidence = max_prob.item()

            return {
                'family': predicted_family,
                'confidence': confidence,
                'method': 'deep_neural_network'
            }

        except Exception as e:
            self.logger.error(f"Error in deep family classification: {e}")
            return {'family': 'unknown', 'confidence': 0.0}

    def _analyze_behavioral_sequence(self, api_calls: List[str]) -> Dict[str, Any]:
        """Analyze behavioral sequence using RNN"""
        if not PYTORCH_AVAILABLE or 'behavioral_rnn' not in self.deep_models or not api_calls:
            return {}

        try:
            model = self.deep_models['behavioral_rnn']
            model.eval()

            # Convert API calls to sequence of indices
            api_sequence = self._api_calls_to_sequence(api_calls)
            if not api_sequence:
                return {}

            # Pad sequence
            sequence_tensor = torch.LongTensor([api_sequence[:self.max_seq_length]]).to(self.device)

            # Get behavioral classification
            with torch.no_grad():
                output = model(sequence_tensor)
                probabilities = F.softmax(output, dim=1)

                # Get behavioral category
                max_prob, predicted_category = torch.max(probabilities, 1)

                behavioral_categories = [
                    'benign', 'trojan', 'ransomware', 'spyware', 'adware',
                    'rootkit', 'worm', 'backdoor', 'bot', 'unknown_malware'
                ]

                category = behavioral_categories[predicted_category.item()]
                confidence = max_prob.item()

            # Extract patterns from sequence analysis
            patterns = self._extract_sequence_patterns(api_calls, category, confidence)

            return {
                'behavioral_category': category,
                'confidence': confidence,
                'patterns': patterns,
                'sequence_length': len(api_sequence)
            }

        except Exception as e:
            self.logger.error(f"Error in behavioral sequence analysis: {e}")
            return {}

    def _calculate_embedding_similarity(self, feature_vector: List[float]) -> Dict[str, float]:
        """Calculate similarity using learned embeddings"""
        if not PYTORCH_AVAILABLE or 'embedding_model' not in self.deep_models:
            return {}

        try:
            model = self.deep_models['embedding_model']
            model.eval()

            # Get embedding for current sample
            features_tensor = torch.FloatTensor([feature_vector]).to(self.device)
            current_embedding = model.get_embedding(features_tensor)

            # Compare with known family embeddings (if available)
            similarity_scores = {}

            # Load reference embeddings if available
            embeddings_file = self.model_dir / "family_embeddings.pt"
            if embeddings_file.exists():
                reference_embeddings = torch.load(embeddings_file, map_location=self.device)

                for family, ref_embedding in reference_embeddings.items():
                    # Calculate cosine similarity
                    similarity = F.cosine_similarity(
                        current_embedding, ref_embedding.unsqueeze(0), dim=1
                    ).item()
                    similarity_scores[f"{family}_embedding"] = similarity

            return similarity_scores

        except Exception as e:
            self.logger.error(f"Error calculating embedding similarity: {e}")
            return {}

    def _ensemble_predictions(self, ml_pred: Dict, deep_pred: Dict = None,
                            behavioral_pred: Dict = None) -> Dict[str, Any]:
        """Ensemble multiple prediction methods"""
        predictions = [ml_pred]
        weights = [0.4]  # Base ML weight

        if deep_pred and deep_pred.get('confidence', 0) > 0.1:
            predictions.append(deep_pred)
            weights.append(0.4)  # Deep learning weight

        if behavioral_pred and behavioral_pred.get('confidence', 0) > 0.1:
            # Convert behavioral category to family if possible
            behavior_family = self._behavioral_to_family(behavioral_pred.get('behavioral_category'))
            if behavior_family:
                predictions.append({
                    'family': behavior_family,
                    'confidence': behavioral_pred['confidence']
                })
                weights.append(0.2)  # Behavioral weight

        # Normalize weights
        total_weight = sum(weights)
        weights = [w / total_weight for w in weights]

        # Weighted ensemble
        family_scores = {}
        for pred, weight in zip(predictions, weights):
            family = pred.get('family', 'unknown')
            confidence = pred.get('confidence', 0.0)

            if family in family_scores:
                family_scores[family] += confidence * weight
            else:
                family_scores[family] = confidence * weight

        # Get best prediction
        if family_scores:
            best_family = max(family_scores, key=family_scores.get)
            best_confidence = family_scores[best_family]
        else:
            best_family = 'unknown'
            best_confidence = 0.0

        return {
            'family': best_family,
            'confidence': best_confidence,
            'method': 'ensemble'
        }

    def _api_calls_to_sequence(self, api_calls: List[str]) -> List[int]:
        """Convert API calls to sequence of indices"""
        # Simple vocabulary mapping (in practice, this would be pre-built)
        api_vocab = getattr(self, 'api_vocab', {})

        if not api_vocab:
            # Build basic vocabulary from common API calls
            common_apis = [
                'CreateFile', 'ReadFile', 'WriteFile', 'DeleteFile', 'CopyFile',
                'CreateProcess', 'OpenProcess', 'TerminateProcess', 'CreateThread',
                'RegOpenKey', 'RegSetValue', 'RegQueryValue', 'RegDeleteKey',
                'socket', 'connect', 'send', 'recv', 'InternetOpen', 'HttpSendRequest',
                'LoadLibrary', 'GetProcAddress', 'VirtualAlloc', 'VirtualProtect',
                'CreateService', 'StartService', 'SetWindowsHook'
            ]

            api_vocab = {api: idx + 1 for idx, api in enumerate(common_apis)}  # 0 reserved for unknown
            self.api_vocab = api_vocab

        # Convert API calls to indices
        sequence = []
        for api in api_calls:
            idx = api_vocab.get(api, 0)  # 0 for unknown APIs
            sequence.append(idx)

        return sequence

    def _extract_sequence_patterns(self, api_calls: List[str], category: str,
                                 confidence: float) -> List[BehavioralPattern]:
        """Extract behavioral patterns from API call sequence"""
        patterns = []

        # Analyze API call patterns
        api_counter = Counter(api_calls)

        # Network communication pattern
        network_apis = ['socket', 'connect', 'send', 'recv', 'InternetOpen', 'HttpSendRequest']
        network_count = sum(api_counter.get(api, 0) for api in network_apis)

        if network_count > 0:
            patterns.append(BehavioralPattern(
                type="network_sequence",
                description=f"Network API sequence detected ({network_count} calls)",
                confidence=min(network_count / 10.0, 1.0),
                mitre_techniques=["T1071", "T1105"]
            ))

        # File manipulation pattern
        file_apis = ['CreateFile', 'ReadFile', 'WriteFile', 'DeleteFile', 'CopyFile']
        file_count = sum(api_counter.get(api, 0) for api in file_apis)

        if file_count > 0:
            patterns.append(BehavioralPattern(
                type="file_manipulation_sequence",
                description=f"File manipulation sequence detected ({file_count} calls)",
                confidence=min(file_count / 15.0, 1.0),
                mitre_techniques=["T1005", "T1083"]
            ))

        # Process manipulation pattern
        process_apis = ['CreateProcess', 'OpenProcess', 'TerminateProcess', 'CreateThread']
        process_count = sum(api_counter.get(api, 0) for api in process_apis)

        if process_count > 0:
            patterns.append(BehavioralPattern(
                type="process_manipulation_sequence",
                description=f"Process manipulation sequence detected ({process_count} calls)",
                confidence=min(process_count / 8.0, 1.0),
                mitre_techniques=["T1055", "T1106"]
            ))

        return patterns

    def _behavioral_to_family(self, behavioral_category: str) -> Optional[str]:
        """Map behavioral category to malware family"""
        category_mapping = {
            'ransomware': 'wannacry',
            'trojan': 'emotet',
            'spyware': 'apt28',
            'backdoor': 'lazarus',
            'bot': 'fin7'
        }

        return category_mapping.get(behavioral_category)

    def _extract_behavioral_patterns(self, features: MalwareFeatures) -> List[BehavioralPattern]:
        """Extract behavioral patterns from features"""
        patterns = []

        if features.network_indicators > 0:
            patterns.append(BehavioralPattern(
                type="network_communication",
                description=f"Network communication indicators: {features.network_indicators}",
                confidence=min(features.network_indicators / 10.0, 1.0),
                mitre_techniques=["T1071", "T1105"]
            ))

        if features.persistence_indicators > 0:
            patterns.append(BehavioralPattern(
                type="persistence",
                description=f"Persistence mechanisms: {features.persistence_indicators}",
                confidence=min(features.persistence_indicators / 5.0, 1.0),
                mitre_techniques=["T1547", "T1053"]
            ))

        if features.evasion_indicators > 0:
            patterns.append(BehavioralPattern(
                type="defense_evasion",
                description=f"Evasion techniques: {features.evasion_indicators}",
                confidence=min(features.evasion_indicators / 5.0, 1.0),
                mitre_techniques=["T1055", "T1140"]
            ))

        if features.crypto_indicators > 0:
            patterns.append(BehavioralPattern(
                type="cryptographic_operations",
                description=f"Cryptographic operations: {features.crypto_indicators}",
                confidence=min(features.crypto_indicators / 3.0, 1.0),
                mitre_techniques=["T1486", "T1027"]
            ))

        return patterns

    def train_classifier(self, training_samples: List[Tuple[str, str, List[str], List[str], Dict]]):
        """Train advanced malware classifier with deep learning models"""
        if not SKLEARN_AVAILABLE:
            self.logger.error("scikit-learn not available for training")
            return False

        try:
            X = []
            y = []
            string_data = []
            api_sequences = []

            # Extract features and prepare data
            for file_path, family, strings, apis, code_analysis in training_samples:
                features = self.feature_extractor.extract_features(
                    file_path, strings, apis, code_analysis
                )
                feature_vector = self._features_to_vector(features)
                X.append(feature_vector)
                y.append(family)
                string_data.append(' '.join(strings) if strings else '')

                # Convert API calls to sequence for RNN training
                if apis:
                    api_sequence = self._api_calls_to_sequence(apis)
                    api_sequences.append(api_sequence)
                else:
                    api_sequences.append([])

            X = np.array(X)
            y = np.array(y)

            # Create family mappings
            unique_families = list(set(y))
            self.family_to_idx = {family: idx for idx, family in enumerate(unique_families)}
            self.idx_to_family = {idx: family for family, idx in self.family_to_idx.items()}
            self.num_families = len(unique_families)

            # Train traditional ML models
            success = self._train_traditional_models(X, y, string_data)
            if not success:
                return False

            # Train deep learning models if available
            if PYTORCH_AVAILABLE and self.deep_models:
                success = self._train_deep_models(X, y, api_sequences)
                if not success:
                    self.logger.warning("Deep learning training failed, continuing with traditional models")

            # Save all models
            self._save_models()

            return True

        except Exception as e:
            self.logger.error(f"Error training advanced classifier: {e}")
            return False

    def _train_traditional_models(self, X: np.ndarray, y: np.ndarray, string_data: List[str]) -> bool:
        """Train traditional ML models"""
        try:
            # Train feature-based classifier
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            family_model = self.models['family_classifier']
            family_model.fit(X_train, y_train)

            train_score = family_model.score(X_train, y_train)
            test_score = family_model.score(X_test, y_test)

            # Train string-based classifier
            if string_data:
                vectorizer = self.vectorizers['string_vectorizer']
                string_features = vectorizer.fit_transform(string_data)

                string_model = self.models['string_classifier']
                string_model.fit(string_features, y)

            # Train anomaly detector on benign samples
            benign_samples = X[y == 'benign'] if 'benign' in y else X[:10]
            anomaly_model = self.models['anomaly_detector']
            anomaly_model.fit(benign_samples)

            # Train behavioral clusterer
            behavior_model = self.models['behavior_clusterer']
            behavior_model.fit(X)

            self.logger.info(f"Traditional ML training completed:")
            self.logger.info(f"  Training accuracy: {train_score:.3f}")
            self.logger.info(f"  Test accuracy: {test_score:.3f}")

            return True

        except Exception as e:
            self.logger.error(f"Error training traditional models: {e}")
            return False

    def _train_deep_models(self, X: np.ndarray, y: np.ndarray, api_sequences: List[List[int]]) -> bool:
        """Train deep learning models"""
        try:
            # Reinitialize models with correct dimensions
            self._initialize_deep_models()

            # Prepare datasets
            dataset = AdvancedMalwareDataset(
                features=X.tolist(),
                labels=y.tolist(),
                sequences=api_sequences,
                family_to_idx=self.family_to_idx
            )

            # Split dataset
            train_size = int(0.8 * len(dataset))
            test_size = len(dataset) - train_size
            train_dataset, test_dataset = torch.utils.data.random_split(
                dataset, [train_size, test_size]
            )

            # Create data loaders
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

            # Train deep classifier
            self._train_deep_classifier(train_loader, test_loader)

            # Train behavioral RNN
            if any(api_sequences):  # Only if we have API sequences
                self._train_behavioral_rnn(train_loader, test_loader)

            # Train embedding model
            self._train_embedding_model(train_loader)

            self.logger.info("Deep learning training completed successfully")
            return True

        except Exception as e:
            self.logger.error(f"Error training deep models: {e}")
            return False

    def _train_deep_classifier(self, train_loader: DataLoader, test_loader: DataLoader, epochs: int = 50):
        """Train deep neural network classifier"""
        model = self.deep_models['deep_classifier']
        optimizer = self.optimizers['deep_classifier']
        criterion = self.loss_functions['classification']

        model.train()

        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            total = 0

            for batch in train_loader:
                features = batch['features'].to(self.device)
                labels = batch['label'].to(self.device)

                optimizer.zero_grad()
                outputs = model(features)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            if epoch % 10 == 0:
                accuracy = 100 * correct / total
                self.logger.info(f"Deep Classifier Epoch {epoch}: Loss={total_loss/len(train_loader):.4f}, Acc={accuracy:.2f}%")

    def _train_behavioral_rnn(self, train_loader: DataLoader, test_loader: DataLoader, epochs: int = 30):
        """Train behavioral analysis RNN"""
        model = self.deep_models['behavioral_rnn']
        optimizer = self.optimizers['behavioral_rnn']
        criterion = nn.CrossEntropyLoss()

        model.train()

        # Create behavioral labels (simplified mapping)
        behavioral_mapping = {
            'benign': 0, 'trojan': 1, 'ransomware': 2, 'spyware': 3, 'adware': 4,
            'rootkit': 5, 'worm': 6, 'backdoor': 7, 'bot': 8, 'unknown': 9
        }

        for epoch in range(epochs):
            total_loss = 0

            for batch in train_loader:
                if 'sequence' not in batch:
                    continue

                sequences = batch['sequence'].to(self.device)
                # Map family labels to behavioral categories (simplified)
                family_labels = batch['label']
                behavioral_labels = torch.zeros_like(family_labels)

                # Simple mapping for demonstration
                for i, family_idx in enumerate(family_labels):
                    family = self.idx_to_family.get(family_idx.item(), 'unknown')
                    if 'ransom' in family.lower():
                        behavioral_labels[i] = behavioral_mapping['ransomware']
                    elif 'trojan' in family.lower() or 'emotet' in family.lower():
                        behavioral_labels[i] = behavioral_mapping['trojan']
                    elif 'apt' in family.lower():
                        behavioral_labels[i] = behavioral_mapping['spyware']
                    else:
                        behavioral_labels[i] = behavioral_mapping['unknown']

                behavioral_labels = behavioral_labels.to(self.device)

                optimizer.zero_grad()
                outputs = model(sequences)
                loss = criterion(outputs, behavioral_labels)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if epoch % 10 == 0:
                self.logger.info(f"Behavioral RNN Epoch {epoch}: Loss={total_loss/len(train_loader):.4f}")

    def _train_embedding_model(self, train_loader: DataLoader, epochs: int = 40):
        """Train embedding model using contrastive learning"""
        model = self.deep_models['embedding_model']
        optimizer = self.optimizers['embedding_model']
        criterion = self.loss_functions['contrastive']

        model.train()

        for epoch in range(epochs):
            total_loss = 0

            for batch in train_loader:
                features = batch['features'].to(self.device)
                labels = batch['label'].to(self.device)

                # Create pairs for contrastive learning
                batch_size = features.size(0)
                if batch_size < 2:
                    continue

                # Simple pairing strategy
                indices1 = torch.arange(0, batch_size - 1)
                indices2 = torch.arange(1, batch_size)

                features1 = features[indices1]
                features2 = features[indices2]
                labels1 = labels[indices1]
                labels2 = labels[indices2]

                # Create similarity labels (1 if same family, 0 if different)
                similarity_labels = (labels1 == labels2).float()

                # Get embeddings
                embeddings1 = model(features1)
                embeddings2 = model(features2)

                optimizer.zero_grad()
                loss = criterion(embeddings1, embeddings2, similarity_labels)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if epoch % 10 == 0:
                self.logger.info(f"Embedding Model Epoch {epoch}: Loss={total_loss/len(train_loader):.4f}")

        # Save family embeddings for similarity comparison
        self._save_family_embeddings(train_loader)

    def _save_family_embeddings(self, train_loader: DataLoader):
        """Save representative embeddings for each family"""
        if 'embedding_model' not in self.deep_models:
            return

        model = self.deep_models['embedding_model']
        model.eval()

        family_embeddings = {}
        family_features = {}

        # Collect features for each family
        with torch.no_grad():
            for batch in train_loader:
                features = batch['features'].to(self.device)
                labels = batch['label']

                for i, label in enumerate(labels):
                    family = self.idx_to_family.get(label.item(), 'unknown')
                    if family not in family_features:
                        family_features[family] = []
                    family_features[family].append(features[i])

        # Calculate average embedding for each family
        for family, feature_list in family_features.items():
            if feature_list:
                # Stack features and get average embedding
                stacked_features = torch.stack(feature_list)
                avg_features = torch.mean(stacked_features, dim=0, keepdim=True)
                avg_embedding = model.get_embedding(avg_features)
                family_embeddings[family] = avg_embedding.squeeze(0)

        # Save embeddings
        embeddings_file = self.model_dir / "family_embeddings.pt"
        torch.save(family_embeddings, embeddings_file)
        self.logger.info(f"Saved family embeddings for {len(family_embeddings)} families")

    def _save_models(self):
        """Save trained models to disk"""
        # Save traditional ML models
        for model_name, model in self.models.items():
            model_file = self.model_dir / f"{model_name}_model.pkl"
            try:
                with open(model_file, 'wb') as f:
                    pickle.dump(model, f)
                self.logger.info(f"Saved model: {model_name}")
            except Exception as e:
                self.logger.error(f"Failed to save model {model_name}: {e}")

        # Save vectorizers
        for vec_name, vectorizer in self.vectorizers.items():
            vec_file = self.model_dir / f"{vec_name}.pkl"
            try:
                with open(vec_file, 'wb') as f:
                    pickle.dump(vectorizer, f)
                self.logger.info(f"Saved vectorizer: {vec_name}")
            except Exception as e:
                self.logger.error(f"Failed to save vectorizer {vec_name}: {e}")

        # Save deep learning models
        if PYTORCH_AVAILABLE and self.deep_models:
            for model_name, model in self.deep_models.items():
                model_file = self.model_dir / f"{model_name}_deep.pt"
                try:
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'model_config': {
                            'input_size': getattr(model, 'input_size', self.feature_dim),
                            'num_families': self.num_families,
                            'embedding_dim': self.embedding_dim
                        }
                    }, model_file)
                    self.logger.info(f"Saved deep model: {model_name}")
                except Exception as e:
                    self.logger.error(f"Failed to save deep model {model_name}: {e}")

        # Save family mappings
        mappings_file = self.model_dir / "family_mappings.json"
        try:
            with open(mappings_file, 'w') as f:
                json.dump({
                    'family_to_idx': self.family_to_idx,
                    'idx_to_family': self.idx_to_family,
                    'num_families': self.num_families
                }, f, indent=2)
            self.logger.info("Saved family mappings")
        except Exception as e:
            self.logger.error(f"Failed to save family mappings: {e}")

    def _load_models(self):
        """Load pre-trained models from disk"""
        # Load traditional ML models
        for model_name in self.models.keys():
            model_file = self.model_dir / f"{model_name}_model.pkl"
            if model_file.exists():
                try:
                    # Use joblib for safer model loading
                    import joblib
                    self.models[model_name] = joblib.load(model_file)
                    self.logger.info(f"Loaded pre-trained model: {model_name}")
                except Exception as e:
                    self.logger.warning(f"Failed to load model {model_name}: {e}")

        # Load vectorizers
        for vec_name in self.vectorizers.keys():
            vec_file = self.model_dir / f"{vec_name}.pkl"
            if vec_file.exists():
                try:
                    # Use joblib for safer vectorizer loading
                    import joblib
                    self.vectorizers[vec_name] = joblib.load(vec_file)
                    self.logger.info(f"Loaded vectorizer: {vec_name}")
                except Exception as e:
                    self.logger.warning(f"Failed to load vectorizer {vec_name}: {e}")

        # Load family mappings
        mappings_file = self.model_dir / "family_mappings.json"
        if mappings_file.exists():
            try:
                with open(mappings_file, 'r') as f:
                    mappings = json.load(f)
                    self.family_to_idx = mappings.get('family_to_idx', {})
                    self.idx_to_family = {int(k): v for k, v in mappings.get('idx_to_family', {}).items()}
                    self.num_families = mappings.get('num_families', 20)
                self.logger.info("Loaded family mappings")
            except Exception as e:
                self.logger.warning(f"Failed to load family mappings: {e}")

        # Load deep learning models
        if PYTORCH_AVAILABLE:
            for model_name in ['deep_classifier', 'behavioral_rnn', 'embedding_model']:
                model_file = self.model_dir / f"{model_name}_deep.pt"
                if model_file.exists():
                    try:
                        checkpoint = torch.load(model_file, map_location=self.device)

                        # Reinitialize model with correct parameters
                        if model_name == 'deep_classifier':
                            model = DeepMalwareClassifier(
                                input_size=self.feature_dim,
                                num_families=self.num_families
                            ).to(self.device)
                        elif model_name == 'behavioral_rnn':
                            model = BehavioralAnalysisRNN(
                                vocab_size=self.vocab_size,
                                embedding_dim=128,
                                hidden_dim=256,
                                num_layers=2,
                                num_classes=10
                            ).to(self.device)
                        elif model_name == 'embedding_model':
                            model = MalwareEmbeddingModel(
                                input_size=self.feature_dim,
                                embedding_dim=self.embedding_dim
                            ).to(self.device)

                        model.load_state_dict(checkpoint['model_state_dict'])
                        self.deep_models[model_name] = model
                        self.logger.info(f"Loaded deep model: {model_name}")

                    except Exception as e:
                        self.logger.warning(f"Failed to load deep model {model_name}: {e}")

    def get_malware_similarity_matrix(self, samples: List[Tuple[str, List[str], List[str], Dict]]) -> np.ndarray:
        """Generate similarity matrix for malware samples using embeddings"""
        if not PYTORCH_AVAILABLE or 'embedding_model' not in self.deep_models:
            self.logger.warning("Embedding model not available for similarity analysis")
            return np.array([])

        try:
            model = self.deep_models['embedding_model']
            model.eval()

            embeddings = []

            # Extract embeddings for all samples
            for file_path, strings, apis, code_analysis in samples:
                features = self.feature_extractor.extract_features(
                    file_path, strings, apis, code_analysis
                )
                feature_vector = self._features_to_vector(features)

                features_tensor = torch.FloatTensor([feature_vector]).to(self.device)
                embedding = model.get_embedding(features_tensor)
                embeddings.append(embedding.cpu().numpy().flatten())

            embeddings = np.array(embeddings)

            # Calculate pairwise cosine similarity
            similarity_matrix = cosine_similarity(embeddings)

            return similarity_matrix

        except Exception as e:
            self.logger.error(f"Error generating similarity matrix: {e}")
            return np.array([])

    def cluster_malware_samples(self, samples: List[Tuple[str, List[str], List[str], Dict]],
                               n_clusters: int = 5) -> Dict[str, Any]:
        """Cluster malware samples using embedding-based similarity"""
        if not SKLEARN_AVAILABLE:
            return {}

        try:
            # Get similarity matrix
            similarity_matrix = self.get_malware_similarity_matrix(samples)
            if similarity_matrix.size == 0:
                return {}

            # Convert similarity to distance and ensure non-negative
            distance_matrix = np.maximum(0, 1 - similarity_matrix)

            # Perform clustering
            clusterer = DBSCAN(metric='precomputed', eps=0.3, min_samples=2)
            cluster_labels = clusterer.fit_predict(distance_matrix)

            # Analyze clusters
            unique_labels = set(cluster_labels)
            clusters = {}

            for label in unique_labels:
                if label == -1:  # Noise points
                    continue

                cluster_indices = [i for i, l in enumerate(cluster_labels) if l == label]
                cluster_samples = [samples[i][0] for i in cluster_indices]  # File paths

                clusters[f"cluster_{label}"] = {
                    'samples': cluster_samples,
                    'size': len(cluster_samples),
                    'indices': cluster_indices
                }

            return {
                'clusters': clusters,
                'cluster_labels': cluster_labels.tolist(),
                'num_clusters': len(clusters),
                'noise_points': sum(1 for l in cluster_labels if l == -1)
            }

        except Exception as e:
            self.logger.error(f"Error clustering malware samples: {e}")
            return {}


def main():
    """Main function for testing advanced ML malware classifier"""
    print("Initializing Advanced ML Malware Classifier...")
    classifier = MLMalwareClassifier()

    # Test classification with advanced features
    test_file = "test_sample.exe"
    test_strings = [
        "http://malicious.com", "CreateProcess", "RegSetValue",
        "InternetConnect", "CryptEncrypt", "VirtualProtect"
    ]
    test_apis = [
        "CreateProcess", "RegSetValue", "InternetConnect", "socket",
        "CryptAcquireContext", "VirtualAlloc", "WriteProcessMemory"
    ]
    test_code = {
        "function_count": 50,
        "import_count": 20,
        "basic_block_count": 500,
        "call_graph_complexity": 0.7
    }

    print("\nTesting advanced malware classification...")
    classification = classifier.classify_malware(
        test_file, test_strings, test_apis, test_code
    )

    print(f"\n=== Classification Results ===")
    print(f"Family: {classification.family}")
    print(f"Confidence: {classification.confidence:.2f}")
    print(f"Is Malware: {classification.is_malware}")
    print(f"Classification Method: {classification.classification_method}")
    print(f"Anomaly Score: {classification.anomaly_score:.2f}")
    print(f"Behavioral Cluster: {classification.behavioral_cluster}")

    print(f"\n=== Behavioral Patterns ===")
    for i, pattern in enumerate(classification.behavioral_patterns):
        print(f"{i+1}. {pattern.type}: {pattern.description} (confidence: {pattern.confidence:.2f})")

    print(f"\n=== Similarity Scores ===")
    for family, score in classification.similarity_scores.items():
        print(f"{family}: {score:.3f}")

    # Test similarity analysis if we have multiple samples
    print(f"\n=== Testing Similarity Analysis ===")
    test_samples = [
        (test_file, test_strings, test_apis, test_code),
        ("sample2.exe", ["http://evil.com", "CreateFile"], ["CreateFile", "RegOpenKey"], {"function_count": 30}),
        ("sample3.exe", ["bitcoin", "encrypt"], ["CryptEncrypt", "CreateProcess"], {"function_count": 40})
    ]

    similarity_matrix = classifier.get_malware_similarity_matrix(test_samples)
    if similarity_matrix.size > 0:
        print("Similarity Matrix:")
        for i, row in enumerate(similarity_matrix):
            print(f"Sample {i+1}: {[f'{val:.3f}' for val in row]}")

    # Test clustering
    clustering_result = classifier.cluster_malware_samples(test_samples)
    if clustering_result:
        print(f"\n=== Clustering Results ===")
        print(f"Number of clusters: {clustering_result.get('num_clusters', 0)}")
        print(f"Noise points: {clustering_result.get('noise_points', 0)}")

        for cluster_name, cluster_info in clustering_result.get('clusters', {}).items():
            print(f"{cluster_name}: {cluster_info['size']} samples")

    print(f"\n=== Deep Learning Status ===")
    print(f"PyTorch Available: {PYTORCH_AVAILABLE}")
    print(f"Transformers Available: {TRANSFORMERS_AVAILABLE}")
    print(f"Deep Models Loaded: {len(classifier.deep_models)}")

    if PYTORCH_AVAILABLE:
        print("Deep learning models:")
        for model_name in classifier.deep_models.keys():
            print(f"  - {model_name}")


if __name__ == "__main__":
    main()
