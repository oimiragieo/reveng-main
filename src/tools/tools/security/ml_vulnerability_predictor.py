#!/usr/bin/env python3
"""
ML-Based Vulnerability Predictor
===============================

Machine learning models for vulnerability prediction in binary analysis.
Uses scikit-learn for traditional ML approaches and feature extraction
pipelines for code analysis.

Author: REVENG Project - AI Enhancement Module
Version: 1.0
"""

import json
import logging
import pickle
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import re

try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learn not available - ML vulnerability prediction disabled")

try:
    from .ai_enhanced_data_models import (
        VulnerabilityPrediction, MLModelResult, FeatureVector,
        Evidence, Severity, EvidenceTracker
    )
    from .vulnerability_dataset_loader import VulnerabilityDatasetLoader
except ImportError:
    from ai_enhanced_data_models import (
        VulnerabilityPrediction, MLModelResult, FeatureVector,
        Evidence, Severity, EvidenceTracker
    )
    from vulnerability_dataset_loader import VulnerabilityDatasetLoader


@dataclass
class VulnerabilityFeatures:
    """Feature vector for vulnerability prediction"""
    # Code complexity features
    cyclomatic_complexity: float = 0.0
    lines_of_code: int = 0
    function_count: int = 0
    nested_loops: int = 0

    # Memory management features
    malloc_count: int = 0
    free_count: int = 0
    pointer_arithmetic: int = 0
    array_access: int = 0

    # String handling features
    strcpy_usage: int = 0
    strcat_usage: int = 0
    sprintf_usage: int = 0
    gets_usage: int = 0

    # Input validation features
    input_functions: int = 0
    validation_checks: int = 0
    sanitization_calls: int = 0

    # Cryptographic features
    crypto_functions: int = 0
    random_functions: int = 0
    hash_functions: int = 0

    # Network features
    socket_operations: int = 0
    network_calls: int = 0
    url_patterns: int = 0

    # File operations
    file_operations: int = 0
    path_traversal_risk: int = 0
    temp_file_usage: int = 0

    # Language-specific features
    language: str = "unknown"
    has_unsafe_casts: bool = False
    has_buffer_operations: bool = False
    has_format_strings: bool = False


class VulnerabilityFeatureExtractor:
    """Extract features from code for vulnerability prediction"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # Define patterns for feature extraction
        self.patterns = {
            'dangerous_functions': {
                'strcpy': r'\bstrcpy\s*\(',
                'strcat': r'\bstrcat\s*\(',
                'sprintf': r'\bsprintf\s*\(',
                'gets': r'\bgets\s*\(',
                'scanf': r'\bscanf\s*\(',
            },
            'memory_functions': {
                'malloc': r'\bmalloc\s*\(',
                'free': r'\bfree\s*\(',
                'calloc': r'\bcalloc\s*\(',
                'realloc': r'\brealloc\s*\(',
            },
            'input_functions': {
                'fgets': r'\bfgets\s*\(',
                'fread': r'\bfread\s*\(',
                'read': r'\bread\s*\(',
                'recv': r'\brecv\s*\(',
            },
            'crypto_functions': {
                'encrypt': r'\b\w*encrypt\w*\s*\(',
                'decrypt': r'\b\w*decrypt\w*\s*\(',
                'hash': r'\b\w*hash\w*\s*\(',
                'random': r'\b\w*rand\w*\s*\(',
            },
            'network_functions': {
                'socket': r'\bsocket\s*\(',
                'connect': r'\bconnect\s*\(',
                'bind': r'\bbind\s*\(',
                'listen': r'\blisten\s*\(',
            },
            'file_functions': {
                'fopen': r'\bfopen\s*\(',
                'open': r'\bopen\s*\(',
                'write': r'\bwrite\s*\(',
                'close': r'\bclose\s*\(',
            }
        }

        # Validation patterns
        self.validation_patterns = [
            r'if\s*\([^)]*\s*!=\s*NULL\s*\)',
            r'if\s*\([^)]*\s*>\s*0\s*\)',
            r'assert\s*\(',
            r'check\w*\s*\(',
            r'validate\w*\s*\(',
        ]

        # Sanitization patterns
        self.sanitization_patterns = [
            r'sanitize\w*\s*\(',
            r'escape\w*\s*\(',
            r'filter\w*\s*\(',
            r'clean\w*\s*\(',
        ]

    def extract_features(self, code: str, language: str = "c") -> VulnerabilityFeatures:
        """Extract vulnerability prediction features from code"""
        features = VulnerabilityFeatures()
        features.language = language

        # Basic code metrics
        features.lines_of_code = len(code.split('\n'))
        features.function_count = len(re.findall(r'\b\w+\s*\([^)]*\)\s*{', code))
        features.nested_loops = self._count_nested_loops(code)
        features.cyclomatic_complexity = self._calculate_cyclomatic_complexity(code)

        # Count dangerous function usage
        for category, patterns in self.patterns.items():
            for func_name, pattern in patterns.items():
                count = len(re.findall(pattern, code, re.IGNORECASE))

                if category == 'dangerous_functions':
                    if func_name == 'strcpy':
                        features.strcpy_usage = count
                    elif func_name == 'strcat':
                        features.strcat_usage = count
                    elif func_name == 'sprintf':
                        features.sprintf_usage = count
                    elif func_name == 'gets':
                        features.gets_usage = count

                elif category == 'memory_functions':
                    if func_name == 'malloc':
                        features.malloc_count = count
                    elif func_name == 'free':
                        features.free_count = count

                elif category == 'input_functions':
                    features.input_functions += count

                elif category == 'crypto_functions':
                    features.crypto_functions += count

                elif category == 'network_functions':
                    features.network_calls += count

                elif category == 'file_functions':
                    features.file_operations += count

        # Count validation and sanitization
        features.validation_checks = sum(
            len(re.findall(pattern, code, re.IGNORECASE))
            for pattern in self.validation_patterns
        )

        features.sanitization_calls = sum(
            len(re.findall(pattern, code, re.IGNORECASE))
            for pattern in self.sanitization_patterns
        )

        # Language-specific features
        features.has_unsafe_casts = bool(re.search(r'\([^)]*\*[^)]*\)', code))
        features.has_buffer_operations = bool(re.search(r'\[\s*\w+\s*\]', code))
        features.has_format_strings = bool(re.search(r'%[sdxo]', code))

        # Additional risk indicators
        features.pointer_arithmetic = len(re.findall(r'\w+\s*[\+\-]\s*\d+', code))
        features.array_access = len(re.findall(r'\w+\s*\[\s*\w+\s*\]', code))
        features.url_patterns = len(re.findall(r'https?://[^\s<>"{}|\\^`\[\]]+', code))
        features.path_traversal_risk = len(re.findall(r'\.\./', code))
        features.temp_file_usage = len(re.findall(r'/tmp/|\\temp\\', code, re.IGNORECASE))

        return features

    def _count_nested_loops(self, code: str) -> int:
        """Count nested loop structures"""
        loop_patterns = [r'\bfor\s*\(', r'\bwhile\s*\(', r'\bdo\s*{']
        nested_count = 0

        lines = code.split('\n')
        loop_depth = 0
        max_depth = 0

        for line in lines:
            # Count loop starts
            for pattern in loop_patterns:
                loop_depth += len(re.findall(pattern, line))

            # Count loop ends (simplified)
            loop_depth -= line.count('}')
            loop_depth = max(0, loop_depth)

            max_depth = max(max_depth, loop_depth)

        return max_depth

    def _calculate_cyclomatic_complexity(self, code: str) -> float:
        """Calculate simplified cyclomatic complexity"""
        # Count decision points
        decision_patterns = [
            r'\bif\s*\(',
            r'\belse\s+if\s*\(',
            r'\bwhile\s*\(',
            r'\bfor\s*\(',
            r'\bswitch\s*\(',
            r'\bcase\s+',
            r'\bcatch\s*\(',
            r'\?\s*.*\s*:',  # ternary operator
        ]

        complexity = 1  # Base complexity

        for pattern in decision_patterns:
            complexity += len(re.findall(pattern, code, re.IGNORECASE))

        return float(complexity)


class MLVulnerabilityPredictor:
    """Machine learning-based vulnerability predictor"""

    def __init__(self, model_dir: str = "models"):
        self.logger = logging.getLogger(__name__)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)

        self.feature_extractor = VulnerabilityFeatureExtractor()
        self.evidence_tracker = EvidenceTracker()

        # Initialize models
        self.models = {}
        self.scalers = {}
        self.vectorizers = {}
        self.model_metadata = {}

        # False positive reduction thresholds (adjusted for initial training)
        self.confidence_thresholds = {
            'buffer_overflow': 0.55,
            'injection': 0.55,
            'memory_corruption': 0.60,
            'general': 0.50
        }

        # Performance tracking
        self.prediction_history = []
        self.false_positive_rate = {}

        if not SKLEARN_AVAILABLE:
            self.logger.error("scikit-learn not available - ML prediction disabled")
            return

        # Initialize model pipelines
        self._initialize_models()

        # Load pre-trained models if available
        self._load_models()

        # Initialize dataset loader
        self.dataset_loader = VulnerabilityDatasetLoader(str(self.model_dir / "datasets"))

        # Load vulnerability datasets
        self._load_vulnerability_datasets()

    def _initialize_models(self):
        """Initialize ML model pipelines with enhanced configurations"""
        if not SKLEARN_AVAILABLE:
            return

        # Buffer overflow prediction model - optimized for memory vulnerabilities
        self.models['buffer_overflow'] = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', RandomForestClassifier(
                n_estimators=150,
                max_depth=12,
                min_samples_split=5,
                min_samples_leaf=2,
                class_weight='balanced',
                random_state=42
            ))
        ])

        # Injection vulnerability model - optimized for input validation issues
        self.models['injection'] = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', GradientBoostingClassifier(
                n_estimators=120,
                learning_rate=0.08,
                max_depth=8,
                subsample=0.8,
                random_state=42
            ))
        ])

        # Memory corruption model - optimized for memory management issues
        self.models['memory_corruption'] = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', LogisticRegression(
                random_state=42,
                max_iter=2000,
                class_weight='balanced',
                C=0.1
            ))
        ])

        # General vulnerability model - ensemble approach
        self.models['general'] = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', RandomForestClassifier(
                n_estimators=250,
                max_depth=18,
                min_samples_split=4,
                min_samples_leaf=1,
                class_weight='balanced',
                random_state=42
            ))
        ])

        # Initialize model metadata
        for model_name in self.models.keys():
            self.model_metadata[model_name] = {
                'trained': False,
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'training_samples': 0,
                'last_trained': None
            }

    def _load_models(self):
        """Load pre-trained models from disk"""
        for model_name in self.models.keys():
            model_file = self.model_dir / f"{model_name}_model.pkl"
            if model_file.exists():
                try:
                    # Use joblib for safer model loading
                    import joblib
                    self.models[model_name] = joblib.load(model_file)
                    self.logger.info(f"Loaded pre-trained model: {model_name}")
                except Exception as e:
                    self.logger.warning(f"Failed to load model {model_name}: {e}")

    def _save_models(self):
        """Save trained models to disk"""
        for model_name, model in self.models.items():
            model_file = self.model_dir / f"{model_name}_model.pkl"
            metadata_file = self.model_dir / f"{model_name}_metadata.json"

            try:
                # Save model
                with open(model_file, 'wb') as f:
                    pickle.dump(model, f)

                # Save metadata
                with open(metadata_file, 'w') as f:
                    json.dump(self.model_metadata[model_name], f, indent=2, default=str)

                self.logger.info(f"Saved model and metadata: {model_name}")
            except Exception as e:
                self.logger.error(f"Failed to save model {model_name}: {e}")

    def _load_vulnerability_datasets(self):
        """Load vulnerability datasets for training"""
        self.vulnerability_datasets = {
            'cve_samples': [],
            'synthetic_samples': [],
            'real_world_samples': []
        }

        try:
            # Load CVE-based samples using dataset loader
            cve_training_data = self.dataset_loader.load_cve_dataset(limit=50)
            self.vulnerability_datasets['cve_samples'] = self.dataset_loader.convert_to_training_format(cve_training_data)
            self.logger.info(f"Loaded {len(self.vulnerability_datasets['cve_samples'])} CVE samples")

            # Load real-world samples
            real_world_records = self.dataset_loader.load_real_world_samples()
            self.vulnerability_datasets['real_world_samples'] = self.dataset_loader.convert_to_training_format(real_world_records)
            self.logger.info(f"Loaded {len(self.vulnerability_datasets['real_world_samples'])} real-world samples")

        except Exception as e:
            self.logger.warning(f"Failed to load external datasets: {e}")

        # Generate synthetic samples
        self.vulnerability_datasets['synthetic_samples'] = self._generate_enhanced_training_data()
        self.logger.info(f"Generated {len(self.vulnerability_datasets['synthetic_samples'])} synthetic samples")

    def predict_vulnerabilities(self, code: str, language: str = "c") -> List[VulnerabilityPrediction]:
        """Predict vulnerabilities in code using ML models with enhanced confidence scoring"""
        if not SKLEARN_AVAILABLE:
            return []

        predictions = []

        try:
            # Extract features
            features = self.feature_extractor.extract_features(code, language)
            feature_vector = self._features_to_vector(features)

            # Make predictions with each model
            for vuln_type, model in self.models.items():
                try:
                    # Skip if model not trained
                    if not self.model_metadata[vuln_type]['trained']:
                        continue

                    # Get prediction probabilities
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba([feature_vector])[0]
                        raw_confidence = float(max(proba))
                        is_vulnerable = bool(model.predict([feature_vector])[0])
                    else:
                        prediction = model.predict([feature_vector])[0]
                        is_vulnerable = bool(prediction)
                        raw_confidence = 0.7  # Default confidence

                    # Apply confidence calibration and false positive reduction
                    calibrated_confidence = self._calibrate_confidence(
                        raw_confidence, vuln_type, features, language
                    )

                    # Apply dynamic threshold based on model performance
                    threshold = self._get_dynamic_threshold(vuln_type)

                    # Debug logging
                    self.logger.debug(f"{vuln_type}: raw={raw_confidence:.3f}, calibrated={calibrated_confidence:.3f}, threshold={threshold:.3f}, vulnerable={is_vulnerable}")

                    # Create prediction if confidence is above threshold
                    if calibrated_confidence > threshold and is_vulnerable:
                        # Additional validation to reduce false positives
                        if self._validate_prediction(vuln_type, features, code):
                            # Create evidence with detailed analysis
                            evidence = self.evidence_tracker.add_evidence(
                                "ml_vulnerability_prediction",
                                f"ML model predicted {vuln_type} vulnerability with {calibrated_confidence:.2f} confidence",
                                "machine_learning",
                                calibrated_confidence,
                                {
                                    "model_type": vuln_type,
                                    "raw_confidence": raw_confidence,
                                    "calibrated_confidence": calibrated_confidence,
                                    "threshold_used": threshold,
                                    "feature_count": len(feature_vector),
                                    "language": language,
                                    "model_accuracy": self.model_metadata[vuln_type]['accuracy']
                                }
                            )

                            prediction = VulnerabilityPrediction(
                                vulnerability_type=vuln_type,
                                confidence=calibrated_confidence,
                                is_vulnerable=is_vulnerable,
                                severity=self._map_severity(vuln_type, calibrated_confidence),
                                description=self._generate_detailed_description(vuln_type, features),
                                features_used=self._get_important_features(features, vuln_type),
                                model_version="2.0",
                                evidence=[evidence]
                            )
                            predictions.append(prediction)

                except Exception as e:
                    self.logger.error(f"Error predicting with {vuln_type} model: {e}")

            # Record prediction for performance tracking
            self._record_prediction(code, language, predictions)

            return predictions

        except Exception as e:
            self.logger.error(f"Error in vulnerability prediction: {e}")
            return []

    def _features_to_vector(self, features: VulnerabilityFeatures) -> List[float]:
        """Convert features to numerical vector"""
        vector = [
            features.cyclomatic_complexity,
            float(features.lines_of_code),
            float(features.function_count),
            float(features.nested_loops),
            float(features.malloc_count),
            float(features.free_count),
            float(features.pointer_arithmetic),
            float(features.array_access),
            float(features.strcpy_usage),
            float(features.strcat_usage),
            float(features.sprintf_usage),
            float(features.gets_usage),
            float(features.input_functions),
            float(features.validation_checks),
            float(features.sanitization_calls),
            float(features.crypto_functions),
            float(features.random_functions),
            float(features.hash_functions),
            float(features.socket_operations),
            float(features.network_calls),
            float(features.url_patterns),
            float(features.file_operations),
            float(features.path_traversal_risk),
            float(features.temp_file_usage),
            float(features.has_unsafe_casts),
            float(features.has_buffer_operations),
            float(features.has_format_strings),
        ]
        return vector

    def _map_severity(self, vuln_type: str, confidence: float) -> Severity:
        """Map vulnerability type and confidence to severity"""
        base_severity = {
            'buffer_overflow': Severity.HIGH,
            'injection': Severity.HIGH,
            'memory_corruption': Severity.CRITICAL,
            'general': Severity.MEDIUM
        }

        severity = base_severity.get(vuln_type, Severity.MEDIUM)

        # Adjust based on confidence
        if confidence > 0.9:
            if severity == Severity.MEDIUM:
                severity = Severity.HIGH
            elif severity == Severity.HIGH:
                severity = Severity.CRITICAL
        elif confidence < 0.7:
            if severity == Severity.CRITICAL:
                severity = Severity.HIGH
            elif severity == Severity.HIGH:
                severity = Severity.MEDIUM

        return severity

    def _calibrate_confidence(self, raw_confidence: float, vuln_type: str,
                             features: VulnerabilityFeatures, language: str) -> float:
        """Calibrate confidence score based on model performance and context"""
        calibrated = raw_confidence

        # Adjust based on model accuracy
        model_accuracy = self.model_metadata[vuln_type]['accuracy']
        if model_accuracy > 0:
            calibrated *= model_accuracy

        # Language-specific adjustments
        if language in ['javascript', 'python']:
            # These languages have different vulnerability patterns
            calibrated *= 0.9
        elif language in ['c', 'cpp']:
            # C/C++ more prone to memory vulnerabilities
            if vuln_type in ['buffer_overflow', 'memory_corruption']:
                calibrated *= 1.1

        # Feature-based adjustments
        if vuln_type == 'buffer_overflow':
            if features.strcpy_usage > 0 or features.gets_usage > 0:
                calibrated *= 1.2  # Strong indicators
            if features.validation_checks > 0:
                calibrated *= 0.8  # Validation reduces risk

        elif vuln_type == 'injection':
            if features.sanitization_calls > 0:
                calibrated *= 0.7  # Sanitization reduces risk
            if features.validation_checks == 0:
                calibrated *= 1.15  # No validation increases risk

        # Ensure confidence stays in valid range
        return min(max(calibrated, 0.0), 1.0)

    def _get_dynamic_threshold(self, vuln_type: str) -> float:
        """Get dynamic threshold based on model performance"""
        base_threshold = self.confidence_thresholds[vuln_type]

        # Adjust based on false positive rate
        if vuln_type in self.false_positive_rate:
            fp_rate = self.false_positive_rate[vuln_type]
            if fp_rate > 0.2:  # High false positive rate
                base_threshold += 0.1
            elif fp_rate < 0.05:  # Low false positive rate
                base_threshold -= 0.05

        return min(max(base_threshold, 0.5), 0.95)

    def _validate_prediction(self, vuln_type: str, features: VulnerabilityFeatures, code: str) -> bool:
        """Additional validation to reduce false positives"""

        if vuln_type == 'buffer_overflow':
            # Check for actual dangerous patterns
            dangerous_patterns = ['strcpy', 'gets', 'sprintf']
            has_dangerous = any(pattern in code.lower() for pattern in dangerous_patterns)
            has_bounds_check = 'sizeof' in code or 'strlen' in code

            # Require dangerous pattern and lack of bounds checking
            return has_dangerous and not has_bounds_check

        elif vuln_type == 'injection':
            # Check for input handling without validation
            has_input = features.input_functions > 0
            has_validation = features.validation_checks > 0 or features.sanitization_calls > 0

            # Require input handling without proper validation
            return has_input and not has_validation

        elif vuln_type == 'memory_corruption':
            # Check for memory management issues
            malloc_free_imbalance = abs(features.malloc_count - features.free_count) > 0
            has_pointer_arithmetic = features.pointer_arithmetic > 0

            return malloc_free_imbalance or has_pointer_arithmetic

        return True  # Default to allowing prediction

    def _generate_detailed_description(self, vuln_type: str, features: VulnerabilityFeatures) -> str:
        """Generate detailed vulnerability description"""
        descriptions = {
            'buffer_overflow': f"Buffer overflow vulnerability detected. Found {features.strcpy_usage} strcpy calls, "
                             f"{features.gets_usage} gets calls, and {features.array_access} array accesses.",

            'injection': f"Injection vulnerability detected. Found {features.input_functions} input functions "
                        f"with {features.validation_checks} validation checks and {features.sanitization_calls} sanitization calls.",

            'memory_corruption': f"Memory corruption vulnerability detected. Found {features.malloc_count} malloc calls, "
                               f"{features.free_count} free calls, and {features.pointer_arithmetic} pointer arithmetic operations.",

            'general': f"General vulnerability detected based on code complexity ({features.cyclomatic_complexity}) "
                      f"and {features.function_count} functions analyzed."
        }

        return descriptions.get(vuln_type, f"Vulnerability of type {vuln_type} detected.")

    def _record_prediction(self, code: str, language: str, predictions: List[VulnerabilityPrediction]):
        """Record prediction for performance tracking"""
        record = {
            'timestamp': str(Path(__file__).stat().st_mtime),  # Simple timestamp
            'language': language,
            'code_length': len(code),
            'predictions_count': len(predictions),
            'vulnerability_types': [p.vulnerability_type for p in predictions],
            'confidence_scores': [p.confidence for p in predictions]
        }

        self.prediction_history.append(record)

        # Keep only last 1000 predictions
        if len(self.prediction_history) > 1000:
            self.prediction_history = self.prediction_history[-1000:]

    def _get_important_features(self, features: VulnerabilityFeatures, vuln_type: str) -> List[str]:
        """Get list of important features for the prediction"""
        important_features = []

        if vuln_type == 'buffer_overflow':
            if features.strcpy_usage > 0:
                important_features.append(f"strcpy usage: {features.strcpy_usage}")
            if features.gets_usage > 0:
                important_features.append(f"gets usage: {features.gets_usage}")
            if features.sprintf_usage > 0:
                important_features.append(f"sprintf usage: {features.sprintf_usage}")
            if features.has_buffer_operations:
                important_features.append("buffer operations detected")
            if features.array_access > 0:
                important_features.append(f"array accesses: {features.array_access}")

        elif vuln_type == 'injection':
            if features.input_functions > 0:
                important_features.append(f"input functions: {features.input_functions}")
            if features.validation_checks == 0:
                important_features.append("no input validation detected")
            if features.sanitization_calls == 0:
                important_features.append("no input sanitization detected")
            if features.has_format_strings:
                important_features.append("format string usage detected")

        elif vuln_type == 'memory_corruption':
            if features.malloc_count != features.free_count:
                important_features.append(f"malloc/free imbalance: {features.malloc_count}/{features.free_count}")
            if features.pointer_arithmetic > 0:
                important_features.append(f"pointer arithmetic: {features.pointer_arithmetic}")
            if features.has_unsafe_casts:
                important_features.append("unsafe type casts detected")

        elif vuln_type == 'general':
            if features.cyclomatic_complexity > 10:
                important_features.append(f"high complexity: {features.cyclomatic_complexity}")
            if features.nested_loops > 2:
                important_features.append(f"nested loops: {features.nested_loops}")
            if features.function_count > 20:
                important_features.append(f"many functions: {features.function_count}")

        return important_features

    def train_model(self, training_data: List[Tuple[str, str, bool]], model_type: str = "general"):
        """Train ML model on vulnerability dataset with enhanced metrics"""
        if not SKLEARN_AVAILABLE:
            self.logger.error("scikit-learn not available for training")
            return False

        try:
            from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
            from datetime import datetime

            # Extract features and labels
            X = []
            y = []

            self.logger.info(f"Processing {len(training_data)} training samples for {model_type}")

            for code, language, is_vulnerable in training_data:
                features = self.feature_extractor.extract_features(code, language)
                feature_vector = self._features_to_vector(features)
                X.append(feature_vector)
                y.append(int(is_vulnerable))

            X = np.array(X)
            y = np.array(y)

            # Check class distribution
            positive_samples = np.sum(y)
            negative_samples = len(y) - positive_samples
            self.logger.info(f"Class distribution: {positive_samples} vulnerable, {negative_samples} safe")

            # Split data with stratification
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            # Train model
            model = self.models[model_type]
            self.logger.info(f"Training {model_type} model...")
            model.fit(X_train, y_train)

            # Comprehensive evaluation
            train_score = model.score(X_train, y_train)
            test_score = model.score(X_test, y_test)

            # Predictions for detailed metrics
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred

            # Calculate detailed metrics
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)

            try:
                auc = roc_auc_score(y_test, y_pred_proba)
            except ValueError:
                auc = 0.0  # Handle case where only one class is present

            # Cross-validation
            cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
            cv_precision = cross_val_score(model, X, y, cv=5, scoring='precision')
            cv_recall = cross_val_score(model, X, y, cv=5, scoring='recall')

            # Update model metadata
            self.model_metadata[model_type].update({
                'trained': True,
                'accuracy': float(test_score),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'auc_score': float(auc),
                'training_samples': len(training_data),
                'last_trained': datetime.now().isoformat(),
                'cv_accuracy_mean': float(cv_scores.mean()),
                'cv_accuracy_std': float(cv_scores.std()),
                'cv_precision_mean': float(cv_precision.mean()),
                'cv_recall_mean': float(cv_recall.mean())
            })

            # Calculate and update false positive rate
            false_positives = np.sum((y_pred == 1) & (y_test == 0))
            total_negatives = np.sum(y_test == 0)
            fp_rate = false_positives / total_negatives if total_negatives > 0 else 0.0
            self.false_positive_rate[model_type] = fp_rate

            # Log comprehensive results
            self.logger.info(f"Model {model_type} training completed:")
            self.logger.info(f"  Training accuracy: {train_score:.3f}")
            self.logger.info(f"  Test accuracy: {test_score:.3f}")
            self.logger.info(f"  Precision: {precision:.3f}")
            self.logger.info(f"  Recall: {recall:.3f}")
            self.logger.info(f"  F1-score: {f1:.3f}")
            self.logger.info(f"  AUC: {auc:.3f}")
            self.logger.info(f"  False Positive Rate: {fp_rate:.3f}")
            self.logger.info(f"  CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

            # Save model and metadata
            self._save_models()

            return True

        except Exception as e:
            self.logger.error(f"Error training model {model_type}: {e}")
            return False

    def _generate_enhanced_training_data(self) -> List[Tuple[str, str, bool]]:
        """Generate comprehensive synthetic training data for model training"""
        training_data = []

        # Buffer overflow vulnerabilities
        buffer_overflow_examples = [
            ("""
            void strcpy_overflow(char *input) {
                char buffer[64];
                strcpy(buffer, input);  // Classic buffer overflow
                process_data(buffer);
            }
            """, "c", True),

            ("""
            void gets_vulnerability() {
                char user_input[128];
                printf("Enter data: ");
                gets(user_input);  // Dangerous gets function
                handle_input(user_input);
            }
            """, "c", True),

            ("""
            void sprintf_overflow(char *name, int age) {
                char message[50];
                sprintf(message, "Hello %s, you are %d years old and this is a very long message", name, age);
                display_message(message);
            }
            """, "c", True),

            ("""
            void array_overflow() {
                int data[10];
                for (int i = 0; i <= 10; i++) {  // Off-by-one error
                    data[i] = i * 2;
                }
            }
            """, "c", True),
        ]

        # Memory corruption vulnerabilities
        memory_corruption_examples = [
            ("""
            void use_after_free() {
                char *ptr = malloc(100);
                free(ptr);
                strcpy(ptr, "data");  // Use after free
            }
            """, "c", True),

            ("""
            void double_free_bug() {
                char *buffer = malloc(256);
                process_buffer(buffer);
                free(buffer);
                if (error_condition) {
                    free(buffer);  // Double free
                }
            }
            """, "c", True),

            ("""
            void memory_leak_loop() {
                for (int i = 0; i < 1000; i++) {
                    char *temp = malloc(1024);
                    if (process_data(temp) < 0) {
                        continue;  // Memory leak in loop
                    }
                    free(temp);
                }
            }
            """, "c", True),

            ("""
            void null_pointer_deref(char *input) {
                char *ptr = NULL;
                if (strlen(input) > 10) {
                    ptr = malloc(100);
                }
                strcpy(ptr, input);  // Potential null pointer dereference
            }
            """, "c", True),
        ]

        # Injection vulnerabilities
        injection_examples = [
            ("""
            void sql_injection_vuln(char *username) {
                char query[512];
                sprintf(query, "SELECT * FROM users WHERE username='%s'", username);
                execute_sql(query);  // SQL injection
            }
            """, "c", True),

            ("""
            void command_injection(char *filename) {
                char command[256];
                sprintf(command, "cat %s", filename);
                system(command);  // Command injection
            }
            """, "c", True),

            ("""
            void format_string_vuln(char *user_data) {
                printf(user_data);  // Format string vulnerability
            }
            """, "c", True),

            ("""
            void path_traversal(char *path) {
                char full_path[512];
                sprintf(full_path, "/var/data/%s", path);  // Path traversal
                FILE *f = fopen(full_path, "r");
            }
            """, "c", True),
        ]

        # Safe code examples
        safe_examples = [
            ("""
            void safe_strcpy(char *input, size_t input_len) {
                char buffer[64];
                if (input_len >= sizeof(buffer)) {
                    log_error("Input too long");
                    return;
                }
                strncpy(buffer, input, sizeof(buffer) - 1);
                buffer[sizeof(buffer) - 1] = '\\0';
                process_data(buffer);
            }
            """, "c", False),

            ("""
            void safe_input_handling() {
                char user_input[128];
                printf("Enter data: ");
                if (fgets(user_input, sizeof(user_input), stdin) != NULL) {
                    user_input[strcspn(user_input, "\\n")] = '\\0';  // Remove newline
                    handle_input(user_input);
                }
            }
            """, "c", False),

            ("""
            void safe_memory_management() {
                char *ptr = malloc(100);
                if (ptr == NULL) {
                    log_error("Memory allocation failed");
                    return;
                }

                if (process_data(ptr) < 0) {
                    free(ptr);
                    return;
                }

                free(ptr);
                ptr = NULL;
            }
            """, "c", False),

            ("""
            void safe_sql_query(char *username) {
                if (!validate_username(username)) {
                    log_error("Invalid username");
                    return;
                }

                char *escaped_username = escape_sql_string(username);
                if (escaped_username) {
                    execute_prepared_statement("SELECT * FROM users WHERE username=?", escaped_username);
                    free(escaped_username);
                }
            }
            """, "c", False),

            ("""
            void safe_array_access() {
                int data[10];
                for (int i = 0; i < 10; i++) {  // Proper bounds checking
                    data[i] = i * 2;
                }
            }
            """, "c", False),

            ("""
            void safe_file_operations(char *filename) {
                if (!validate_filename(filename)) {
                    return;
                }

                char sanitized_path[256];
                if (sanitize_path(filename, sanitized_path, sizeof(sanitized_path)) == 0) {
                    FILE *f = fopen(sanitized_path, "r");
                    if (f) {
                        // Process file
                        fclose(f);
                    }
                }
            }
            """, "c", False),
        ]

        # JavaScript vulnerabilities
        js_examples = [
            ("""
            function xss_vulnerability(userInput) {
                document.getElementById('output').innerHTML = userInput;  // XSS
            }
            """, "javascript", True),

            ("""
            function eval_injection(userCode) {
                eval(userCode);  // Code injection
            }
            """, "javascript", True),

            ("""
            function safe_output(userInput) {
                document.getElementById('output').textContent = userInput;  // Safe
            }
            """, "javascript", False),
        ]

        # Python vulnerabilities
        python_examples = [
            ("""
            def sql_injection(username):
                query = f"SELECT * FROM users WHERE username='{username}'"  # SQL injection
                execute_query(query)
            """, "python", True),

            ("""
            def command_injection(filename):
                os.system(f"cat {filename}")  # Command injection
            """, "python", True),

            ("""
            def safe_sql_query(username):
                query = "SELECT * FROM users WHERE username=?"
                execute_query(query, (username,))  # Safe parameterized query
            """, "python", False),
        ]

        # Combine all examples
        training_data.extend(buffer_overflow_examples)
        training_data.extend(memory_corruption_examples)
        training_data.extend(injection_examples)
        training_data.extend(safe_examples)
        training_data.extend(js_examples)
        training_data.extend(python_examples)

        return training_data

    def generate_synthetic_training_data(self) -> List[Tuple[str, str, bool]]:
        """Generate synthetic training data for backward compatibility"""
        return self._generate_enhanced_training_data()


    def get_model_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report for all models"""
        report = {
            'models': {},
            'overall_stats': {
                'total_predictions': len(self.prediction_history),
                'false_positive_rates': self.false_positive_rate.copy(),
                'confidence_thresholds': self.confidence_thresholds.copy()
            }
        }

        for model_name, metadata in self.model_metadata.items():
            report['models'][model_name] = metadata.copy()

        return report

    def update_false_positive_feedback(self, vuln_type: str, was_false_positive: bool):
        """Update false positive tracking based on user feedback"""
        if vuln_type not in self.false_positive_rate:
            self.false_positive_rate[vuln_type] = 0.0

        # Simple exponential moving average for false positive rate
        alpha = 0.1
        current_rate = self.false_positive_rate[vuln_type]

        if was_false_positive:
            self.false_positive_rate[vuln_type] = current_rate + alpha * (1.0 - current_rate)
        else:
            self.false_positive_rate[vuln_type] = current_rate * (1.0 - alpha)

        # Adjust threshold based on false positive rate
        if self.false_positive_rate[vuln_type] > 0.3:
            self.confidence_thresholds[vuln_type] = min(0.9, self.confidence_thresholds[vuln_type] + 0.05)
        elif self.false_positive_rate[vuln_type] < 0.05:
            self.confidence_thresholds[vuln_type] = max(0.5, self.confidence_thresholds[vuln_type] - 0.02)

    def train_all_models(self) -> Dict[str, bool]:
        """Train all models with comprehensive dataset"""
        results = {}

        # Get all available training data
        all_training_data = []

        # Add synthetic data
        all_training_data.extend(self.vulnerability_datasets['synthetic_samples'])

        # Add CVE data if available
        all_training_data.extend(self.vulnerability_datasets['cve_samples'])

        # Add real-world samples if available
        all_training_data.extend(self.vulnerability_datasets['real_world_samples'])

        self.logger.info(f"Training all models with {len(all_training_data)} total samples")

        # Train each model
        for model_type in self.models.keys():
            self.logger.info(f"Training {model_type} model...")
            success = self.train_model(all_training_data, model_type)
            results[model_type] = success

        return results


def main():
    """Main function for testing ML vulnerability predictor"""
    predictor = MLVulnerabilityPredictor()

    # Generate and train on comprehensive synthetic data
    print("Generating comprehensive training data...")
    training_data = predictor.generate_synthetic_training_data()
    print(f"Generated {len(training_data)} training samples")

    print("\nTraining ML models...")
    results = predictor.train_all_models()

    for model_type, success in results.items():
        print(f"Model {model_type}: {'✓' if success else '✗'}")

    # Get performance report
    print("\nModel Performance Report:")
    report = predictor.get_model_performance_report()

    for model_name, stats in report['models'].items():
        if stats['trained']:
            print(f"\n{model_name.upper()} Model:")
            print(f"  Accuracy: {stats['accuracy']:.3f}")
            print(f"  Precision: {stats['precision']:.3f}")
            print(f"  Recall: {stats['recall']:.3f}")
            print(f"  F1-Score: {stats['f1_score']:.3f}")
            print(f"  Training Samples: {stats['training_samples']}")

    # Test prediction with multiple examples
    test_cases = [
        ("Buffer Overflow Test", """
        void test_function(char *input) {
            char buffer[50];
            strcpy(buffer, input);  // Vulnerable
            printf("%s", buffer);
        }
        """),

        ("Memory Leak Test", """
        void memory_function() {
            char *ptr = malloc(100);
            if (error_condition) {
                return;  // Memory leak
            }
            free(ptr);
        }
        """),

        ("Safe Code Test", """
        void safe_function(char *input, size_t len) {
            char buffer[50];
            if (len >= sizeof(buffer)) return;
            strncpy(buffer, input, sizeof(buffer) - 1);
            buffer[sizeof(buffer) - 1] = '\\0';
            printf("%s", buffer);
        }
        """)
    ]

    print("\nTesting vulnerability prediction...")
    for test_name, test_code in test_cases:
        print(f"\n{test_name}:")
        predictions = predictor.predict_vulnerabilities(test_code)

        if predictions:
            for pred in predictions:
                print(f"  - {pred.vulnerability_type}: {pred.confidence:.2f} confidence")
                print(f"    Severity: {pred.severity}")
                print(f"    Key features: {pred.features_used[:2]}")
        else:
            print("  - No vulnerabilities detected")

    print(f"\nTotal predictions made: {len(predictor.prediction_history)}")
    print("ML Vulnerability Predictor test completed!")


if __name__ == "__main__":
    main()
